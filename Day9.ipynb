{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question and Answering 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we will follow [Tunstall, von Werra, and Wolf](https://github.com/nlp-with-transformers)'s Question and Answering chapter today. Since this will involve two new libraries, we will mostly use their code. We will not do the whole chapter today, however, leaving the more advanced subjects of evaluating reciever and reader seperately and fine-tuning a QA pipeline for tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We start importing the QA dataset, based on Amazon's reviews of electronics products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hlovisie/venvs/torch-cuda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "subjqa = load_dataset(\"subjqa\", name=\"electronics\", trust_remote_code = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play a around a little to get a sense of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'domain': 'electronics',\n",
       " 'nn_mod': 'great',\n",
       " 'nn_asp': 'bass response',\n",
       " 'query_mod': 'excellent',\n",
       " 'query_asp': 'bass',\n",
       " 'q_reviews_id': '0514ee34b672623dff659334a25b599b',\n",
       " 'question_subj_level': 5,\n",
       " 'ques_subj_score': 0.5,\n",
       " 'is_ques_subjective': False,\n",
       " 'review_id': '882b1e2745a4779c8f17b3d4406b91c7',\n",
       " 'id': '2543d296da9766d8d17d040ecc781699',\n",
       " 'title': 'B00001P4ZH',\n",
       " 'context': 'I have had Koss headphones in the past, Pro 4AA and QZ-99.  The Koss Portapro is portable AND has great bass response.  The work great with my Android phone and can be \"rolled up\" to be carried in my motorcycle jacket or computer bag without getting crunched.  They are very light and do not feel heavy or bear down on your ears even after listening to music with them on all day.  The sound is night and day better than any ear-bud could be and are almost as good as the Pro 4AA.  They are \"open air\" headphones so you cannot match the bass to the sealed types, but it comes close. For $32, you cannot go wrong.',\n",
       " 'question': 'How is the bass?',\n",
       " 'answers': {'text': [],\n",
       "  'answer_start': [],\n",
       "  'answer_subj_level': [],\n",
       "  'ans_subj_score': [],\n",
       "  'is_ans_subjective': []}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjqa['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one has no answers, so let us look at the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'domain': 'electronics',\n",
       " 'nn_mod': 'harsh',\n",
       " 'nn_asp': 'high',\n",
       " 'query_mod': 'not strong',\n",
       " 'query_asp': 'bass',\n",
       " 'q_reviews_id': '7c46670208f7bf5497480fbdbb44561a',\n",
       " 'question_subj_level': 1,\n",
       " 'ques_subj_score': 0.5,\n",
       " 'is_ques_subjective': False,\n",
       " 'review_id': 'ce76793f036494eabe07b33a9a67288a',\n",
       " 'id': 'd476830bf9282e2b9033e2bb44bbb995',\n",
       " 'title': 'B00001P4ZH',\n",
       " 'context': 'To anyone who hasn\\'t tried all the various types of headphones, it is important to remember exactly what these are: cheap portable on-ear headphones. They give a totally different sound then in-ears or closed design phones, but for what they are I would say they\\'re good. I currently own six pairs of phones, from stock apple earbuds to Sennheiser HD 518s. Gave my Portapros a run on both my computer\\'s sound card and mp3 player, using 256 kbps mp3s or better. The clarity is good and they\\'re very lightweight. The folding design is simple but effective. The look is certainly retro and unique, although I didn\\'t find it as comfortable as many have claimed. Earpads are *very* thin and made my ears sore after 30 minutes of listening, although this can be remedied to a point by adjusting the \"comfort zone\" feature (tightening the temple pads while loosening the ear pads). The cord seems to be an average thickness, but I wouldn\\'t get too rough with these. The steel headband adjusts smoothly and easily, just watch out that the slider doesn\\'t catch your hair. Despite the sore ears, the phones are very lightweight overall.Back to the sound: as you would expect, it\\'s good for a portable phone, but hardly earth shattering. At flat EQ the clarity is good, although the highs can sometimes be harsh. Bass is weak as expected, even with EQ adjusted up. To be fair, a portable on-ear would have a tough time comparing to the bass of an in-ear with a good seal or a pair with larger drivers. No sound isolation offered if you\\'re into that sort of thing. Cool 80s phones, though I\\'ve certainly owned better portable on-ears (Sony makes excellent phones in this category). Soundstage is very narrow and lacks body. A good value if you can get them for under thirty, otherwise I\\'d rather invest in a nicer pair of phones. If we\\'re talking about value, they\\'re a good buy compared to new stock apple buds. If you\\'re trying to compare the sound quality of this product to serious headphones, there\\'s really no comparison at all.Update: After 100 hours of burn-in time the sound has not been affected in any appreciable way. Highs are still harsh, and bass is still underwhelming. I sometimes use these as a convenience but they have been largely replaced in my collection.',\n",
       " 'question': 'Is this music song have a goo bass?',\n",
       " 'answers': {'text': ['Bass is weak as expected',\n",
       "   'Bass is weak as expected, even with EQ adjusted up'],\n",
       "  'answer_start': [1302, 1302],\n",
       "  'answer_subj_level': [1, 1],\n",
       "  'ans_subj_score': [0.5083333253860474, 0.5083333253860474],\n",
       "  'is_ans_subjective': [True, True]}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjqa['train'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better, it has two answers. See how `title` both of them is the same? This is because the product is the same. Let us look at the answers. We can give a look at the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Bass is weak as expected',\n",
       "  'Bass is weak as expected, even with EQ adjusted up'],\n",
       " 'answer_start': [1302, 1302],\n",
       " 'answer_subj_level': [1, 1],\n",
       " 'ans_subj_score': [0.5083333253860474, 0.5083333253860474],\n",
       " 'is_ans_subjective': [True, True]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjqa['train'][1]['answers']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The review itself is *not* the answers, but it is the context. The answers are, in fact, gathered by annotators and correspond to parts of the context. In particular, we see that these answers start at the character of index 1302. We can check that by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First answer: Bass is weak as expected\n",
      "Second answer: Bass is weak as expected, even with EQ adjusted up\n"
     ]
    }
   ],
   "source": [
    "len1, len2 = tuple(map(len, subjqa['train'][1]['answers']['text']))\n",
    "print('First answer:', subjqa['train'][1]['context'][1302 : 1302 + len1])\n",
    "print('Second answer:', subjqa['train'][1]['context'][1302 : 1302 + len2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Let us move to the actual fun part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move to the actual haystack solution, let us use Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/minilm-uncased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# We will take a model pretrained on SquAD 2, a famous QA dataset\n",
    "model_ckpt = \"deepset/minilm-uncased-squad2\"\n",
    "pipe = pipeline(\"question-answering\", model = model_ckpt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bass is weak as expected\n",
      "Bass is weak\n",
      "Bass is weak as expected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hlovisie/venvs/torch-cuda/lib/python3.11/site-packages/transformers/pipelines/question_answering.py:326: UserWarning: topk parameter is deprecated, use top_k instead\n",
      "  warnings.warn(\"topk parameter is deprecated, use top_k instead\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "question = subjqa['train'][1]['question']\n",
    "context = subjqa['train'][1]['context']\n",
    "k = 3\n",
    "\n",
    "results = pipe(question = question, context = context, topk = k)\n",
    "for i in range(k):\n",
    "    print(results[i]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hlovisie/venvs/torch-cuda/lib/python3.11/site-packages/transformers/pipelines/question_answering.py:326: UserWarning: topk parameter is deprecated, use top_k instead\n",
      "  warnings.warn(\"topk parameter is deprecated, use top_k instead\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.25434815883636475,\n",
       "  'start': 1302,\n",
       "  'end': 1326,\n",
       "  'answer': 'Bass is weak as expected'},\n",
       " {'score': 0.23717091977596283,\n",
       "  'start': 1302,\n",
       "  'end': 1314,\n",
       "  'answer': 'Bass is weak'},\n",
       " {'score': 0.20091791450977325,\n",
       "  'start': 1302,\n",
       "  'end': 1326,\n",
       "  'answer': 'Bass is weak as expected'}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(question = question, context = context, topk = k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, but let us evaluate the whole zero-shot model. We will use squad metric for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "squad_metric = evaluate.load(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 2.793296089385475, 'f1': 12.891644169881934}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = subjqa['test']\n",
    "preds = []\n",
    "refs = []\n",
    "for rev in test_data:\n",
    "    pred = pipe(question = rev['question'], context = rev['context'])\n",
    "    # Cannot evaluate with an empty string, so we give it a dummy\n",
    "    if len(pred['answer']) == 0:\n",
    "        cpred = ''\n",
    "    else:\n",
    "        cpred = pred['answer']\n",
    "    \n",
    "    preds.append({\n",
    "        'id': rev['id'],\n",
    "        'prediction_text': cpred\n",
    "    })\n",
    "    \n",
    "    if len(rev['answers']['text']) == 0:\n",
    "        ctext = ['']\n",
    "        cstart = [0]\n",
    "    else:\n",
    "        ctext = rev['answers']['text']\n",
    "        cstart = rev['answers']['answer_start']\n",
    "\n",
    "    refs.append({\n",
    "        'id': rev['id'],\n",
    "        'answers': {'text': ctext, \n",
    "                    'answer_start': cstart}\n",
    "    })\n",
    "\n",
    "squad_metric.compute(predictions = preds, references = refs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are percentages (so things are not good). We will need some fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haystack, the library that we will use for QA requires a document store. We will start installing this document store (which, in our case, following the book and my own personal experience, we use Elasticsearch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'f197774c6f58', 'cluster_name': 'docker-cluster', 'cluster_uuid': 'tb75MDvrTUauVE-GKzCJyw', 'version': {'number': '7.9.2', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': 'd34da0ea4a966c4e49417f2da2f244e3e97b4e6e', 'build_date': '2020-09-23T00:45:33.626720Z', 'build_snapshot': False, 'lucene_version': '8.6.2', 'minimum_wire_compatibility_version': '6.8.0', 'minimum_index_compatibility_version': '6.0.0-beta1'}, 'tagline': 'You Know, for Search'}\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "print(es.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_stores import ElasticsearchDocumentStore\n",
    "\n",
    "# Connect document store\n",
    "document_store = ElasticsearchDocumentStore(return_embedding = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will save the documents in the Elasticsearch server, basically following the group syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1615 documents\n"
     ]
    }
   ],
   "source": [
    "# Transform into a pandas dataset (it can be done without it,\n",
    "# but means changing a little the code) for drop duplicates\n",
    "import pandas as pd\n",
    "dfs = {split: dset.to_pandas() for split, dset in subjqa.flatten().items()}\n",
    "# Haystack now asks for a Document class for creating documents\n",
    "from haystack import Document\n",
    "\n",
    "# It's a good idea to flush Elasticsearch with each notebook restart\n",
    "if len(document_store.get_all_documents()) or len(document_store.get_all_labels()) > 0:\n",
    "    document_store.delete_documents(\"document\")\n",
    "    document_store.delete_documents(\"label\")\n",
    "     \n",
    "\n",
    "for split, df in dfs.items():\n",
    "    # Exclude duplicate reviews\n",
    "    docs = [Document(content =  row[\"context\"], \n",
    "             meta = {\"item_id\": row[\"title\"], \"question_id\": row[\"id\"], \n",
    "                     \"split\": split})\n",
    "        for _,row in df.drop_duplicates(subset=\"context\").iterrows()]\n",
    "    document_store.write_documents(\n",
    "        docs, \n",
    "        index = \"document\" # The name of the table (think as SQL)\n",
    "        )\n",
    "print(f\"Loaded {document_store.get_document_count()} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us give a look at one of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: I have had Koss headphones in the past, Pro 4AA and QZ-99.  The Koss Portapro is portable AND has great bass response.  The work great with my Android phone and can be \"rolled up\" to be carried in my motorcycle jacket or computer bag without getting crunched.  They are very light and do not feel heavy or bear down on your ears even after listening to music with them on all day.  The sound is night and day better than any ear-bud could be and are almost as good as the Pro 4AA.  They are \"open air\" headphones so you cannot match the bass to the sealed types, but it comes close. For $32, you cannot go wrong.\n",
      "Metadata: {'item_id': 'B00001P4ZH', 'question_id': '2543d296da9766d8d17d040ecc781699', 'split': 'train'}\n"
     ]
    }
   ],
   "source": [
    "docs = document_store.get_all_documents(index = \"document\", batch_size = 1)\n",
    "print(\"Content:\", docs[0].content)\n",
    "print(\"Metadata:\", docs[0].meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, in general, haystack administrates an embedding of the documents. For example, we can see its dimension by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_store.embedding_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding: None\n"
     ]
    }
   ],
   "source": [
    "print(\"Embedding:\", docs[0].embedding)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what is going on? Nothing! We have not yet specified an embedding for the documents, 768 here is just a placeholder (as most people use BERT-like models for embedding anyways)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us retrieve some elements of our dataset. For such, we will use haysack using BM25 keyword search (very sparse, which will make it quick for playing around)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first need to initialize the retriever\n",
    "from haystack.nodes import BM25Retriever\n",
    "es_retriever = BM25Retriever(document_store = document_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can choose one item (which will be usef to filter the datset) and then retrieve some answers fo a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First retrived: This is a gift to myself\n",
      " I have been a kindle user for 4 years and this is my third one\n",
      " I never thought I would want a fire for I mainly use it for book reading\n",
      " I decided to try the fire for when I travel I take my laptop, my phone and my iPod classic\n",
      " I love my iPod but watching movies on the plane with it can be challenging because it is so small\n",
      "Laptops battery life is not as good as the Kindle\n",
      " So the Fire combines for me what I needed all three to do\n",
      "So far so good.\n",
      "-----------\n",
      "Second retrived: Plays Netflix great, WiFi capability has great range\n",
      "Resolution on the screen is AMAZING! For the price you cannot go wrong\n",
      "Bought one for my spouse and myself after becoming addicted to hers! Our son LOVES it and it is great for reading books when no light is available\n",
      "Amazing sound but I suggest good headphones to really hear it all.Battery life is super long and can go 3 or 4 days without a recharge from moderate use.A steal at $199.99.\n",
      "-----------\n",
      "Third retrived: I've used an e-reader since the Rocket ebook in 1999, and I've always believed I wanted only an ebook reader with no other features\n",
      "I don't get out much, so my computer is convenient for everything else.I've had several Kindles and was happy with all of them, but I had recently played around with my brother's tablet\n",
      "When my last Kindle died, I decided to replace it with the Kindle Fire\n",
      "I chose the smaller one because I was concerned about my arthritic hands holding a larger device for long periods\n",
      "The 7\" is just perfect for me\n",
      "It's light enough that I can hold it to read, but the larger screen compared to the Kindle makes for easier reading\n",
      "I love the color, something I never thought would make a difference to me.Oddly enough, high on my favorite list is the ability to review a book as soon as I reach the end\n",
      "I have written reviews of every book I read on a readers social media site, but I have seldom put in the extra effort to come to Amazon to write a review\n",
      "On the Kindle Fire, as soon as I come to the end of a book, a review page pops up so I can review it while it's fresh on my mind, and I'm reviewing everything I read.I didn't realize when I placed my order that I was ordering that the device had special offers\n",
      "I've always thought I would not want to be subject to advertising when I was reading\n",
      "However, I discovered that the special offers are discreet and no distraction at all\n",
      "I've even found myself going to the special offers page a time or two to see what's on offer.The only negative is that the battery doesn't last very long\n",
      "However, with the PowerFast charger, it doesn't take long to charge the battery.I think folks like me who have always used a dedicated ereader and never even used a touch screen will be pleasantly surprised with the Kindle Fire, and people who are used to smartphones and tablets will find everything they expect in a device.Update 11/8/13: I still think the Kindle Fire is a fantastic product, and I use it occasionally for games\n",
      "However, I have gone back to my Kindle Keyboard for reading novels\n",
      "The arthritis in my shoulders, elbows, and hands has  worsened since I've been using the Kindle Fire, and even using the device for a short while increases the pain significantly\n",
      "I can tolerate it for short periods to play a few games\n",
      "However, I simply can't hold the Kindle Fire long enough for reading\n",
      "This won't be an issue for most people, but I wanted to mention it for those people who might have similar problems.\n"
     ]
    }
   ],
   "source": [
    "item_id = \"B0074BW614\"\n",
    "query = \"Is it good for reading?\"\n",
    "retrieved_docs = es_retriever.retrieve(\n",
    "    query = query, top_k = 3, filters={\"item_id\":[item_id], \"split\":[\"train\"]})\n",
    "\n",
    "print('First retrived:', retrieved_docs[0].content.replace('. ', '\\n'))\n",
    "print('-----------\\nSecond retrived:', retrieved_docs[1].content.replace('. ', '\\n'))\n",
    "print('-----------\\nThird retrived:', retrieved_docs[2].content.replace('. ', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super cool!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us move to the reader, whose goal is exactly to read the retrieved documents and find the answer to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/minilm-uncased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from haystack.nodes import TransformersReader\n",
    "\n",
    "model_ckpt = \"deepset/minilm-uncased-squad2\"\n",
    "max_seq_length = 384\n",
    "doc_stride = 128\n",
    "\n",
    "reader = TransformersReader(\n",
    "    model_name_or_path = model_ckpt,  # only this is needed\n",
    "    max_seq_len = max_seq_length,\n",
    "    doc_stride = doc_stride,\n",
    "    use_gpu = True  # set to False if no GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is it good for reading? \n",
      "\n",
      "Answers:\n",
      "it is great for reading books when no light is available\n",
      "I mainly use it for book reading\n",
      "the larger screen compared to the Kindle makes for easier reading\n"
     ]
    }
   ],
   "source": [
    "k = 3\n",
    "\n",
    "print('Question:', query, '\\n\\nAnswers:')\n",
    "\n",
    "for i in range(k):  \n",
    "\n",
    "    print(reader.predict(query = query, documents = retrieved_docs, top_k =  k)['answers'][i].answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad. But let us make a whole QA pipeline and evluate on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever + Reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a pipeline now is very easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "pipe = ExtractiveQAPipeline(reader = reader, retriever = es_retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All good, let us test a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is it good for reading? \n",
      "\n",
      "Answer 1: it is great for reading books when no light is available\n",
      "Answer 2: I mainly use it for book reading\n",
      "Answer 3: the larger screen compared to the Kindle makes for easier reading\n"
     ]
    }
   ],
   "source": [
    "preds = pipe.run(query = query, \n",
    "                 params = {\n",
    "                     \"Retriever\": {\"top_k\": k,\n",
    "                                    \"filters\": {\"item_id\": [item_id], \"split\":[\"train\"]}}, \n",
    "                                    \"Reader\": {\"top_k\": k}\n",
    "                                    }\n",
    "                                    )\n",
    "\n",
    "print(f\"Question: {preds['query']} \\n\")\n",
    "for i in range(k):\n",
    "    print(f\"Answer {i+1}: {preds['answers'][i].answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad, but it ass for a more quantitive analysis. Let us again use the squad metric!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hlovisie/venvs/torch-cuda/lib/python3.11/site-packages/transformers/pipelines/question_answering.py:395: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match': 3.910614525139665, 'f1': 9.633795746112037}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = subjqa['test']\n",
    "preds = []\n",
    "refs = []\n",
    "for rev in test_data:\n",
    "    pred = pipe.run(query = rev['question'], \n",
    "                 params = {\n",
    "                     \"Retriever\": {\"top_k\": k,\n",
    "                                    \"filters\": {\"item_id\": [rev['title']], \"split\":[\"test\"]}}, \n",
    "                                    \"Reader\": {\"top_k\": 1}\n",
    "                                    }\n",
    "                                    )\n",
    "    pred = pred['answers']\n",
    "    # Cannot evaluate with an empty string, so we give it a dummy\n",
    "    if pred == []:\n",
    "        pred = ''\n",
    "    else:\n",
    "        pred =  pred[0].answer\n",
    "    \n",
    "    \n",
    "    preds.append({\n",
    "        'id': rev['id'],\n",
    "        'prediction_text': pred\n",
    "    })\n",
    "    \n",
    "    if len(rev['answers']['text']) == 0:\n",
    "        ctext = ['']\n",
    "        cstart = [0]\n",
    "    else:\n",
    "        ctext = rev['answers']['text']\n",
    "        cstart = rev['answers']['answer_start']\n",
    "\n",
    "    refs.append({\n",
    "        'id': rev['id'],\n",
    "        'answers': {'text': ctext, \n",
    "                    'answer_start': cstart}\n",
    "    })\n",
    "\n",
    "squad_metric.compute(predictions = preds, references = refs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worse! But notice that here, we are in the much harder retrieval context. In fact, increasing k should lead to improvements in performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hlovisie/venvs/torch-cuda/lib/python3.11/site-packages/transformers/pipelines/question_answering.py:395: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match': 3.910614525139665, 'f1': 9.725831352702185}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 10\n",
    "preds = []\n",
    "refs = []\n",
    "for rev in test_data:\n",
    "    pred = pipe.run(query = rev['question'], \n",
    "                 params = {\n",
    "                     \"Retriever\": {\"top_k\": k,\n",
    "                                    \"filters\": {\"item_id\": [rev['title']], \"split\":[\"test\"]}}, \n",
    "                                    \"Reader\": {\"top_k\": 1}\n",
    "                                    }\n",
    "                                    )\n",
    "    pred = pred['answers']\n",
    "    # Cannot evaluate with an empty string, so we give it a dummy\n",
    "    if pred == []:\n",
    "        pred = ''\n",
    "    else:\n",
    "        pred =  pred[0].answer\n",
    "    \n",
    "    \n",
    "    preds.append({\n",
    "        'id': rev['id'],\n",
    "        'prediction_text': pred\n",
    "    })\n",
    "    \n",
    "    if len(rev['answers']['text']) == 0:\n",
    "        ctext = ['']\n",
    "        cstart = [0]\n",
    "    else:\n",
    "        ctext = rev['answers']['text']\n",
    "        cstart = rev['answers']['answer_start']\n",
    "\n",
    "    refs.append({\n",
    "        'id': rev['id'],\n",
    "        'answers': {'text': ctext, \n",
    "                    'answer_start': cstart}\n",
    "    })\n",
    "\n",
    "squad_metric.compute(predictions = preds, references = refs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, only a marginal improvement (EM was actually the same). This probably indicates that the problem is with our reader, but we will try to tackle this problem tomorrow. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
