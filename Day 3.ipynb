{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will do some improvements on yesterday's model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "We will be using the same data loading startegy, but instead of the very sparse one-hot encoding, we will use TF-IDF for vectorization. We will use slearn's implementation, which means that we do not need tokenization (this is means that we do not need tokenization.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first test the vectorizer. We will use some of its optional arguments to look for better results. In particular, we will experiment with removing stop words. Moreover, we shall use normalization to avoid problems in the NN's gradient learning. We will also restrcit the dimensions so not have overflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = 'english', norm = 'l2', max_features = 10000)\n",
    "text = ['i love you so much', 'he loves her a lot as well']\n",
    "X = vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 3 stored elements and shape (2, 3)>\n",
      "  Coords\tValues\n",
      "  (0, 1)\t1.0\n",
      "  (1, 2)\t0.7071067811865476\n",
      "  (1, 0)\t0.7071067811865476\n",
      "['lot' 'love' 'loves']\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most words were removed for being stop words. We can see that without their removal, the vocabulary (and, consequently, the vectors' dimension) become much bigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 10 stored elements and shape (2, 10)>\n",
      "  Coords\tValues\n",
      "  (0, 4)\t0.5\n",
      "  (0, 9)\t0.5\n",
      "  (0, 7)\t0.5\n",
      "  (0, 6)\t0.5\n",
      "  (1, 1)\t0.408248290463863\n",
      "  (1, 5)\t0.408248290463863\n",
      "  (1, 2)\t0.408248290463863\n",
      "  (1, 3)\t0.408248290463863\n",
      "  (1, 0)\t0.408248290463863\n",
      "  (1, 8)\t0.408248290463863\n",
      "['as' 'he' 'her' 'lot' 'love' 'loves' 'much' 'so' 'well' 'you']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = None, norm = 'l2', max_features = 10000)\n",
    "text = ['i love you so much', 'he loves her a lot as well']\n",
    "X = vectorizer.fit_transform(text)\n",
    "print(X)\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is hard to decide only based on these artificial examples whether to use, but we will try with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "The data loader class changes very little, except, of course, for the vectorization part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, review_df, stop_word = True):\n",
    "        # Load and split data\n",
    "        self.review_df = review_df\n",
    "\n",
    "        self.train_df = self.review_df[self.review_df['split'] == 'train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.review_df[self.review_df['split'] == 'val']\n",
    "        self.val_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.review_df[self.review_df['split'] == 'test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        # We now create the attributes that will be used to interact with each of these datasets\n",
    "        self._lookup_dict = {\n",
    "            'train' : (self.train_df, self.train_size),\n",
    "            'val' : (self.val_df, self.val_size),\n",
    "            'test' : (self.test_df, self.test_size) \n",
    "        }\n",
    "\n",
    "        # This will be defined below, but basically allows for choosing an external dataset at each time\n",
    "        # By default, this is train\n",
    "        # It is important to do so because the __len__ and __get_item__ methods are defined on a fixed\n",
    "        # external data\n",
    "        self.set_split('train') \n",
    "\n",
    "        # Because we will be using the sklearn vectorization, we will need\n",
    "        # to fit it to train from this point on\n",
    "        if stop_word:\n",
    "            stop_word = 'english'\n",
    "        else:\n",
    "            stop_word = None\n",
    "        self.vectorizer = TfidfVectorizer(stop_words = stop_word, norm = 'l2', max_features = 10000)\n",
    "        # We fit the vectorizer only in the train set\n",
    "        self.vectorizer.fit(self.train_df['review'])\n",
    "\n",
    "    def set_split(self, split):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "    \n",
    "    \n",
    "    # This method will be useful for generating an object of the class directly from the csv path\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, review_csv, stop_word = True):\n",
    "        '''\n",
    "        Loads the csv directly and makes a vectorizer based on it. \n",
    "        The vectorizer is naturally constructed only over the train set.\n",
    "        '''\n",
    "        review_df = pd.read_csv(review_csv)\n",
    "        return cls(review_df, stop_word)\n",
    "    \n",
    "    # Here are the important methods so the training loop does work\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        This basically allows one to iterate over the rows of the csv.\n",
    "        '''\n",
    "        row = self._target_df.iloc[index]\n",
    "        # Vectorize the row\n",
    "        review_vector = self.vectorizer.transform([row['review']]).toarray()\n",
    "        mapper =  {'negative' : 0, 'positive' : 1}\n",
    "        rating_index = mapper[row['rating']]\n",
    "        return {\n",
    "            'x_data': review_vector,\n",
    "            'y_data': rating_index,\n",
    "            'text': row['review']\n",
    "            }\n",
    "    \n",
    "    def get_num_batches(self, batch_size):\n",
    "        '''\n",
    "        Returns the number of batches needed for that particular batch size.\n",
    "        '''\n",
    "        return len(self)//batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10000)\n"
     ]
    }
   ],
   "source": [
    "review = ReviewDataset.load_dataset_and_make_vectorizer(\"reviews_with_splits_lite.csv\")\n",
    "encoded = review[0]\n",
    "print(encoded['x_data'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All good! We will again implement the batch generator, but this time, we allow for GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle = True,\n",
    "                     drop_last = True, device = \"cpu\"):\n",
    "    dataloader = DataLoader(dataset = dataset, batch_size = batch_size,\n",
    "                            shuffle = shuffle, drop_last = drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, _ in data_dict.items():\n",
    "            if name != 'text':\n",
    "                out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement a class for computing early stopping with a minimum error and patience. In case of early stopping, we would like to return to the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience = 5, min_delta = 0, save = None):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.save = save\n",
    "        self.best_loss = np.inf\n",
    "        self.patience_counter = 0\n",
    "        self.flag = False\n",
    "    \n",
    "    def __call__(self, val_loss, model = None):\n",
    "        # If the validation loss improved, we basically \n",
    "        # update the new best lost and save the model\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.patience_counter = 0\n",
    "        \n",
    "            if (self.save) and (model is not None):\n",
    "                torch.save(model.state_dict(), self.save)\n",
    "        \n",
    "        else:\n",
    "            self.patience_counter += 1\n",
    "            if self.patience_counter > self.patience:\n",
    "                self.flag = True\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not change much of the model, except that for now, we allow dropout for regularization (we saw that our original training was overfitting a lot). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dimension, dropout_prob = 0.5):\n",
    "        super().__init__() \n",
    "        self.fc1 = nn.Linear(input_size, hidden_dimension)\n",
    "        self.fc2 = nn.Linear(hidden_dimension, 1)\n",
    "        self.dropout = nn.Dropout(p = dropout_prob)\n",
    "        self.input_size = input_size\n",
    "    \n",
    "    def forward(self, x_in):\n",
    "        if x_in.shape[-1] != self.input_size:\n",
    "            print(x_in.shape)\n",
    "            raise Exception(\"Input dimension of the object must be equal to the model's expected diemension!\") \n",
    "        intermediate = F.relu(self.fc1(x_in))\n",
    "        # Add dropout\n",
    "        intermediate = self.dropout(intermediate)\n",
    "        y_out = self.fc2(intermediate)\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we will have some metrics implemented to check the model performance. For that we will use sklearn metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, while we can compute running accuracy, F1 is not an average, so we need to compute on the whole evaluation dataset. This will be done below. Another nice inclusion we will do is establishing a learning schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Training loss 0.6407634097766254\n",
      "Validation loss 0.5343649387359619\n",
      "Validation accuracy:  0.8700721153846154\n",
      "Validation F1:  0.8595921548253019\n",
      "Epoch:  1\n",
      "Training loss 0.4799858628729589\n",
      "Validation loss 0.3869459927082062\n",
      "Validation accuracy:  0.9001201923076924\n",
      "Validation F1:  0.8976474935336864\n",
      "Epoch:  2\n",
      "Training loss 0.39671912789344765\n",
      "Validation loss 0.3153764605522156\n",
      "Validation accuracy:  0.9068509615384616\n",
      "Validation F1:  0.9058208773848584\n",
      "Epoch:  3\n",
      "Training loss 0.3508301390931494\n",
      "Validation loss 0.27585718035697937\n",
      "Validation accuracy:  0.9106971153846154\n",
      "Validation F1:  0.9106863805745883\n",
      "Epoch:  4\n",
      "Training loss 0.3266943512009638\n",
      "Validation loss 0.25198763608932495\n",
      "Validation accuracy:  0.9115384615384615\n",
      "Validation F1:  0.9120458891013384\n",
      "Epoch:  5\n",
      "Training loss 0.3088339102618833\n",
      "Validation loss 0.24987995624542236\n",
      "Validation accuracy:  0.9120192307692307\n",
      "Validation F1:  0.9122091628687935\n",
      "Epoch:  6\n",
      "Training loss 0.30722826646239154\n",
      "Validation loss 0.247682124376297\n",
      "Validation accuracy:  0.9127403846153846\n",
      "Validation F1:  0.9128660585693711\n",
      "Epoch:  7\n",
      "Training loss 0.3063323074598718\n",
      "Validation loss 0.24644094705581665\n",
      "Validation accuracy:  0.9126201923076923\n",
      "Validation F1:  0.912714611598031\n",
      "Epoch:  8\n",
      "Training loss 0.3065795028229165\n",
      "Validation loss 0.24447636306285858\n",
      "Validation accuracy:  0.9128605769230769\n",
      "Validation F1:  0.9128710491527461\n",
      "Epoch:  9\n",
      "Training loss 0.30149564677788526\n",
      "Validation loss 0.24232661724090576\n",
      "Validation accuracy:  0.9140625\n",
      "Validation F1:  0.9140934759101286\n",
      "Epoch:  10\n",
      "Training loss 0.29914350870972356\n",
      "Validation loss 0.24205683171749115\n",
      "Validation accuracy:  0.9140625\n",
      "Validation F1:  0.9141141141141141\n",
      "Epoch:  11\n",
      "Training loss 0.30135060057920565\n",
      "Validation loss 0.2414165586233139\n",
      "Validation accuracy:  0.9145432692307692\n",
      "Validation F1:  0.9146561037090385\n",
      "Epoch:  12\n",
      "Training loss 0.2992692979332666\n",
      "Validation loss 0.24198950827121735\n",
      "Validation accuracy:  0.9140625\n",
      "Validation F1:  0.9140315017434171\n",
      "Epoch:  13\n",
      "Training loss 0.3018583956302384\n",
      "Validation loss 0.24190817773342133\n",
      "Validation accuracy:  0.9140625\n",
      "Validation F1:  0.9141347424042272\n",
      "Epoch:  14\n",
      "Training loss 0.2988552659068232\n",
      "Validation loss 0.24154283106327057\n",
      "Validation accuracy:  0.9141826923076923\n",
      "Validation F1:  0.9142651296829971\n",
      "Early stopped at epoch: 15\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "lr = 1e-3\n",
    "hidden_dimension = 5\n",
    "n_epochs = 50\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "batch_size = 128\n",
    "# Allows for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "classifier = MLP(10000, hidden_dimension, 0.5)\n",
    "classifier.to(device)\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr = 1e-3)\n",
    "# Multiplies the learning rate by `gamma` every `step_size` epoch\n",
    "scheduler = StepLR(optimizer, step_size = 5, gamma = 0.1)\n",
    "\n",
    "early_stopping = EarlyStopping(min_delta = 0.001, save = 'best_mlp_model.pt')\n",
    "\n",
    "for epoch in range(n_epochs): \n",
    "    review.set_split('train')\n",
    "    batch_generator = generate_batches(review, batch_size = batch_size, device = device)\n",
    "    running_loss = 0.\n",
    "    classifier.train()\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # 1. Zero the gradient\n",
    "        classifier.zero_grad()\n",
    "\n",
    "        # 2. Prediction\n",
    "        y_pred = classifier(x_in = batch_dict['x_data'].float()).squeeze()\n",
    "\n",
    "        # 3. Compute loss\n",
    "        loss = loss_func(y_pred, batch_dict['y_data'].float())\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "\n",
    "        # 4. Backpropagate\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Optimize\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluation part, we don't want paramereres to change\n",
    "    classifier.eval()\n",
    "    review.set_split('val')\n",
    "    \n",
    "    # Will be used to store predictions, as F1 can only\n",
    "    # be computed at the end (no running F1)\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    batch_generator = generate_batches(review, batch_size = batch_size, device = device)\n",
    "    for _, batch_dict in enumerate(batch_generator):\n",
    "        with torch.no_grad(): # Avoids backpropagation\n",
    "            all_preds.append(classifier(x_in = batch_dict['x_data'].float()).squeeze().cpu())\n",
    "            all_labels.append(batch_dict['y_data'].cpu())\n",
    "            \n",
    "    # Computes metrics\n",
    "    all_labels, all_preds = torch.cat(all_labels), torch.cat(all_preds)\n",
    "    val_loss = loss_func(all_preds, all_labels.float())\n",
    "    # For computing accuracy and F1, we need the variables as binary\n",
    "    all_preds = (torch.sigmoid(all_preds)>0.5).long()\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "    # Updates early stop\n",
    "    early_stopping(val_loss, classifier)\n",
    "    if early_stopping.flag:\n",
    "        print('Early stopped at epoch:', epoch)\n",
    "        break\n",
    "\n",
    "    # Updates scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    print('Epoch: ', epoch)\n",
    "    print('Training loss', running_loss)\n",
    "    print('Validation loss', val_loss.item())\n",
    "    print('Validation accuracy: ', acc)\n",
    "    print('Validation F1: ', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the best model so we can evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7149/742055418.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  classifier.load_state_dict(torch.load(\"best_mlp_model.pt\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=10000, out_features=5, bias=True)\n",
       "  (fc2): Linear(in_features=5, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.load_state_dict(torch.load(\"best_mlp_model.pt\"))\n",
    "classifier.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.9143028846153847\n",
      "Test F1:  0.9139823862950899\n"
     ]
    }
   ],
   "source": [
    "classifier.eval()\n",
    "review.set_split('test')\n",
    "\n",
    "# Will be used to store predictions, as F1 can only\n",
    "# be computed at the end (no running F1)\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "\n",
    "batch_generator = generate_batches(review, batch_size = batch_size, device = device)\n",
    "for _, batch_dict in enumerate(batch_generator):\n",
    "    with torch.no_grad(): # Avoids backpropagation\n",
    "        all_preds.append(classifier(x_in = batch_dict['x_data'].float()).squeeze().cpu())\n",
    "        all_labels.append(batch_dict['y_data'].cpu())\n",
    "        \n",
    "# Computes metrics\n",
    "all_labels, all_preds = torch.cat(all_labels), torch.cat(all_preds)\n",
    "val_loss = loss_func(all_preds, all_labels.float())\n",
    "# For computing accuracy and F1, we need the variables as binary\n",
    "all_preds = (torch.sigmoid(all_preds)>0.5).long()\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "print('Test accuracy: ', acc)\n",
    "print('Test F1: ', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also apply to individaul reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The review 'i love this place' is POSITIVE with probability 0.965078592300415\n"
     ]
    }
   ],
   "source": [
    "def predict_rating(text, classifier, vectorizer, decision_threshold=0.5):  \n",
    "    vectorized_review = torch.tensor(review.vectorizer.transform([text]).toarray()).to(device)\n",
    "    classifier.eval()\n",
    "    result = classifier(vectorized_review.float())\n",
    "    \n",
    "    probability_value = F.sigmoid(result).cpu().item()\n",
    "    index = 1\n",
    "    if probability_value < decision_threshold:\n",
    "        index = 0\n",
    "\n",
    "    mapper = {'0': 'NEGATIVE', '1': 'POSITIVE'}\n",
    "    return mapper[str(index)], probability_value\n",
    "\n",
    "test_review = \"i love this place\"\n",
    "\n",
    "prediction, probability = predict_rating(test_review, classifier, vectorizer, decision_threshold=0.5)\n",
    "print(\"The review '{}' is {}\".format(test_review, prediction), 'with probability', probability)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
