{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset available in the [original author's Drive](https://drive.google.com/file/d/1Lmv4rsJiCWVs1nzs4ywA9YI-ADsTf6WB/view)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>terrible place to work for i just heard a stor...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>hours , minutes total time for an extremely s...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>my less than stellar review is for service . w...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>i m granting one star because there s no way t...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>the food here is mediocre at best . i went aft...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55995</th>\n",
       "      <td>positive</td>\n",
       "      <td>great food . wonderful , friendly service . i ...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55996</th>\n",
       "      <td>positive</td>\n",
       "      <td>charlotte should be the new standard for moder...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55997</th>\n",
       "      <td>positive</td>\n",
       "      <td>get the encore sandwich ! ! make sure to get i...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55998</th>\n",
       "      <td>positive</td>\n",
       "      <td>i m a pretty big ice cream gelato fan . pretty...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55999</th>\n",
       "      <td>positive</td>\n",
       "      <td>where else can you find all the parts and piec...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         rating                                             review  split\n",
       "0      negative  terrible place to work for i just heard a stor...  train\n",
       "1      negative   hours , minutes total time for an extremely s...  train\n",
       "2      negative  my less than stellar review is for service . w...  train\n",
       "3      negative  i m granting one star because there s no way t...  train\n",
       "4      negative  the food here is mediocre at best . i went aft...  train\n",
       "...         ...                                                ...    ...\n",
       "55995  positive  great food . wonderful , friendly service . i ...   test\n",
       "55996  positive  charlotte should be the new standard for moder...   test\n",
       "55997  positive  get the encore sandwich ! ! make sure to get i...   test\n",
       "55998  positive  i m a pretty big ice cream gelato fan . pretty...   test\n",
       "55999  positive  where else can you find all the parts and piec...   test\n",
       "\n",
       "[56000 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"reviews_with_splits_lite.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main focus of today, understanding the training pipeline and how to use torch's Dataset and DataLoader classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "We will start building the vocabulary constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, token_to_idx = None, add_unk = True, unk_token = '<UNK>'):\n",
    "        '''\n",
    "        token_to_idx is a dictionary that maps each token to an integer. \n",
    "        If the token has never been seen before, we map it `unk_token`.\n",
    "        '''\n",
    "        if token_to_idx is None:\n",
    "            # Allows for general initalization\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        # Creates a dictionary with pointers in the inverse direction\n",
    "        self._idx_to_token = {idx: token\n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "\n",
    "        # We do not use -1 for the unknown index, but will update it later\n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "\n",
    "    # The original implementation does this for serralization, \n",
    "    # but again, I'm not too worried about caching for now\n",
    "    def to_dictionary(self):\n",
    "        return {\n",
    "            'token_to_idx': self._token_to_idx,\n",
    "            'idx_to_token': self._idx_to_token\n",
    "        }\n",
    "    \n",
    "    def add_token(self, token):\n",
    "        '''\n",
    "        Adds one new token.\n",
    "        '''\n",
    "        # If the token already exists, we do nothing.\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            # Otherwise the token is just appended as a list\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def add_many(self, tokens):\n",
    "        '''\n",
    "        Add many tokens in a single run.\n",
    "        '''\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        '''\n",
    "        Returns the token's index.\n",
    "        '''\n",
    "        if self.unk_index >= 0:\n",
    "            # If the unknown has not being added, we return -1 if\n",
    "            # the word is unknown.\n",
    "            # In particular, .get(value, default) avoids raising\n",
    "            # an error if value is not in the dictionary. In that case,\n",
    "            # returns default\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "    \n",
    "    def lookup_index(self, index):\n",
    "        '''\n",
    "        Get a token associated to some index.\n",
    "        '''\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError('The index (%d) is not yet in the vocabulary' % index)\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary (size = %d)>\" % len(self)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play a little with the class to understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary (size = 1)>\n"
     ]
    }
   ],
   "source": [
    "voc = Vocabulary()\n",
    "print(voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary (size = 4)>\n",
      "{'token_to_idx': {'<UNK>': 0, 'i': 1, 'love': 2, 'you': 3}, 'idx_to_token': {0: '<UNK>', 1: 'i', 2: 'love', 3: 'you'}}\n"
     ]
    }
   ],
   "source": [
    "words = ['i', 'love', 'you']\n",
    "voc.add_many(words)\n",
    "print(voc)\n",
    "print(voc.to_dictionary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check the indices and words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "print(voc.lookup_token('i'))\n",
    "print(voc.lookup_token('her'))\n",
    "print(voc.lookup_index(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move on to construct the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "class ReviewVectorizer:\n",
    "    def __init__(self, review_vocab, rating_vocab):\n",
    "        '''\n",
    "        Here, we will make transform sentences/reviews to vectors. \n",
    "        Notice that we also assume that the class labels have been transformed into integers.\n",
    "        '''\n",
    "        self.review_vocab = review_vocab\n",
    "        self.rating_vocab = rating_vocab\n",
    "\n",
    "    def vectorize(self, review):\n",
    "        '''\n",
    "        Here we will use one-hot encoding for vectorization.\n",
    "        '''\n",
    "        one_hot = np.zeros(len(self.review_vocab), dtype = np.float32)\n",
    "\n",
    "        for token in review.split(\" \"):\n",
    "            # We do not encode punctuation\n",
    "            if token not in string.punctuation:\n",
    "                one_hot[self.review_vocab.lookup_token(token)] = 1\n",
    "        return one_hot\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, review_df, cutoff = 25):\n",
    "        '''\n",
    "        Allow the whole vectorization process from a pd DataFrame.\n",
    "        Cutoff makes that words whose frequency are less than that \n",
    "        value are not encoded.\n",
    "        '''\n",
    "        review_vocab = Vocabulary(add_unk=True)\n",
    "        rating_vocab = Vocabulary(add_unk= False)\n",
    "\n",
    "        for rating in sorted(set(review_df.rating)):\n",
    "            rating_vocab.add_token(rating)\n",
    "        \n",
    "        word_counts = Counter()\n",
    "        for review in review_df['review']:\n",
    "            for word in review.split(\" \"):\n",
    "                if word not in string.punctuation:\n",
    "                    word_counts[word] += 1\n",
    "        \n",
    "        # Now we only add words whose frequency are bigger than cutoff.\n",
    "        for word, count in word_counts.items():\n",
    "            if count > cutoff:\n",
    "                review_vocab.add_token(word)\n",
    "        \n",
    "        return cls(review_vocab, rating_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us again play a little with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = ReviewVectorizer(voc, None)\n",
    "vectorizer.vectorize('i hate you so much .')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Makes sense as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_to_idx': {'<UNK>': 0, 'i': 1, 'love': 2, 'you': 3}, 'idx_to_token': {0: '<UNK>', 1: 'i', 2: 'love', 3: 'you'}}\n"
     ]
    }
   ],
   "source": [
    "print(voc.to_dictionary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make the class that will generate the whole dataset. We will use Torch's Dataset for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, review_df, vectorizer):\n",
    "        self.review_df = review_df\n",
    "        self.vectorizer = vectorizer # I have this public instead of private, which avoids\n",
    "                                    # an extra method (I like working with attributes ;p )\n",
    "\n",
    "        # Now we make the split into train, validation, and test\n",
    "        self.train_df = self.review_df[self.review_df['split'] == 'train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.review_df[self.review_df['split'] == 'val']\n",
    "        self.val_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.review_df[self.review_df['split'] == 'test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        # We now create the attributes that will be used to interact with each of these datasets\n",
    "        self._lookup_dict = {\n",
    "            'train' : (self.train_df, self.train_size),\n",
    "            'val' : (self.val_df, self.val_size),\n",
    "            'test' : (self.test_df, self.test_size) \n",
    "        }\n",
    "\n",
    "        # This will be defined below, but basically allows for choosing an external dataset at each time\n",
    "        # By default, this is train\n",
    "        # It is important to do so because the __len__ and __get_item__ methods are defined on a fixed\n",
    "        # external data\n",
    "        self.set_split('train') \n",
    "\n",
    "    def set_split(self, split):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "    \n",
    "    \n",
    "    # This method will be useful for generating an object of the class directly from the csv path\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, review_csv):\n",
    "        '''\n",
    "        Loads the csv directly and makes a vectorizer based on it. \n",
    "        The vectorizer is naturally constructed only over the train set.\n",
    "        '''\n",
    "        review_df = pd.read_csv(review_csv)\n",
    "        train_data = review_df[review_df['split'] == 'train']\n",
    "        return cls(review_df, ReviewVectorizer.from_dataframe(train_data))\n",
    "    \n",
    "    # Here are the important methods so the training loop does work\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        This basically allows one to iterate over the rows of the csv.\n",
    "        '''\n",
    "        row = self._target_df.iloc[index] # Recall that we are basically working with pd DataFrame\n",
    "        # Vectorize the row\n",
    "        review_vector = self.vectorizer.vectorize(row['review'])\n",
    "        rating_index = self.vectorizer.rating_vocab.lookup_token(row['rating'])\n",
    "        return {\n",
    "            'x_data': review_vector,\n",
    "            'y_data': rating_index\n",
    "            }\n",
    "    \n",
    "    def get_num_batches(self, batch_size):\n",
    "        '''\n",
    "        Returns the number of batches needed for that particular batch size.\n",
    "        '''\n",
    "        return len(self)//batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try creating the class, just to check if everything is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "     rating                                             review  split\n",
      "0  negative  terrible place to work for i just heard a stor...  train\n",
      "1  negative   hours , minutes total time for an extremely s...  train\n",
      "2  negative  my less than stellar review is for service . w...  train\n",
      "3  negative  i m granting one star because there s no way t...  train\n",
      "4  negative  the food here is mediocre at best . i went aft...  train\n",
      "{'x_data': array([1., 1., 1., ..., 0., 0., 0.], shape=(8945,), dtype=float32), 'y_data': 0}\n"
     ]
    }
   ],
   "source": [
    "vectorizer = ReviewVectorizer.from_dataframe(data)\n",
    "print(vectorizer.vectorize('i hate this restaurant .'))\n",
    "review = ReviewDataset(data, vectorizer)\n",
    "print(review.train_df.head())\n",
    "print(review[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also load directly from the csv path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     rating                                             review  split\n",
      "0  negative  terrible place to work for i just heard a stor...  train\n",
      "1  negative   hours , minutes total time for an extremely s...  train\n",
      "2  negative  my less than stellar review is for service . w...  train\n",
      "3  negative  i m granting one star because there s no way t...  train\n",
      "4  negative  the food here is mediocre at best . i went aft...  train\n",
      "{'x_data': array([1., 1., 1., ..., 0., 0., 0.], shape=(7326,), dtype=float32), 'y_data': 0}\n"
     ]
    }
   ],
   "source": [
    "review = ReviewDataset.load_dataset_and_make_vectorizer(\"reviews_with_splits_lite.csv\")\n",
    "print(review.train_df.head())\n",
    "print(review[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we do the DataLoader. It's objective is to create the batches for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle = True, drop_last = True):\n",
    "    dataloader = DataLoader(dataset = dataset, \n",
    "                            batch_size = batch_size, \n",
    "                            shuffle = shuffle,\n",
    "                            drop_last = drop_last)\n",
    "    \n",
    "    # I will only be using cpu this time, so no device menaging is necessary\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define training. Instead of using the more complicated training loop (this will be the topic of tomorrow), I'll just use the training loop of yesterday's implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    Perceptron as a class. In general, we like to have models as classes in Torch.\n",
    "    '''\n",
    "    def __init__(self, input_size, hidden_dimension):\n",
    "        super().__init__() \n",
    "        self.fc1 = nn.Linear(input_size, hidden_dimension)\n",
    "        self.fc2 = nn.Linear(hidden_dimension, 1)\n",
    "        self.input_size = input_size\n",
    "    \n",
    "    def forward(self, x_in, apply_sigmoid = False):\n",
    "        if x_in.shape[1] != self.input_size:\n",
    "            raise Exception(\"Input dimension of the object must be equal to the model's expected diemension!\") \n",
    "        intermediate = F.relu(self.fc1(x_in))\n",
    "        y_out = self.fc2(intermediate)\n",
    "        # For computing the cross-entropy loss, we \n",
    "        if apply_sigmoid:\n",
    "            y_out = torch.sigmoid(y_out)\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is for computing the accuracy. I basically copied the original implementation, removing the device stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_target):\n",
    "    y_pred_indices = (torch.sigmoid(y_pred)>0.5).long()\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Training loss 0.4013539999236466\n",
      "Validation accuracy:  91.6826923076923\n",
      "Epoch:  1\n",
      "Training loss 0.22029797099774182\n",
      "Validation accuracy:  92.13942307692305\n",
      "Epoch:  2\n",
      "Training loss 0.17796104303956814\n",
      "Validation accuracy:  92.47596153846156\n",
      "Epoch:  3\n",
      "Training loss 0.15636351838513138\n",
      "Validation accuracy:  92.35576923076921\n",
      "Epoch:  4\n",
      "Training loss 0.1415344056448126\n",
      "Validation accuracy:  92.12740384615385\n",
      "Epoch:  5\n",
      "Training loss 0.13060202868350987\n",
      "Validation accuracy:  91.88701923076923\n",
      "Epoch:  6\n",
      "Training loss 0.12188972649621027\n",
      "Validation accuracy:  91.82692307692311\n",
      "Epoch:  7\n",
      "Training loss 0.11521302539782183\n",
      "Validation accuracy:  91.63461538461539\n",
      "Epoch:  8\n",
      "Training loss 0.10985134475030538\n",
      "Validation accuracy:  91.35817307692305\n",
      "Epoch:  9\n",
      "Training loss 0.10437196455408936\n",
      "Validation accuracy:  91.45432692307692\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "hidden_dimension = 5\n",
    "lr = 0.001\n",
    "n_epochs = 10\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "vectorizer = review.vectorizer\n",
    "classifier = MLP(len(vectorizer.review_vocab), hidden_dimension)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(classifier.parameters(), lr = lr)\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs): \n",
    "    review.set_split('train')\n",
    "    batch_generator = generate_batches(review, batch_size = batch_size)\n",
    "    running_loss = 0.\n",
    "    classifier.train()\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # 1. Zero the gradient\n",
    "        classifier.zero_grad()\n",
    "\n",
    "        # 2. Prediction\n",
    "        y_pred = classifier(x_in = batch_dict['x_data'].float()).squeeze()\n",
    "\n",
    "        # 3. Compute loss\n",
    "        loss = loss_func(y_pred, batch_dict['y_data'].float())\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "\n",
    "        # 4. Backpropagate\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Optimize\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluation part, we don't want paramereres to change\n",
    "    classifier.eval()\n",
    "    review.set_split('val')\n",
    "    batch_generator = generate_batches(review, batch_size = batch_size)\n",
    "    running_acc = 0.\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "        # compute the output\n",
    "        y_pred = classifier(x_in = batch_dict['x_data'].float()).squeeze()\n",
    "\n",
    "        # compute the accuracy\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict['y_data'])\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "    \n",
    "\n",
    "    print('Epoch: ', epoch)\n",
    "    print('Training loss', running_loss)\n",
    "    print('Validation accuracy: ', running_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see some very strong overfitting, but again, this will be treated tomorrow. For now, let us follow the original implementation in doing some qualitative tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(review, classifier, vectorizer, decision_threshold=0.5):  \n",
    "    vectorized_review = torch.tensor(vectorizer.vectorize(review))\n",
    "    classifier.eval()\n",
    "    result = classifier(vectorized_review.view(1, -1))\n",
    "    \n",
    "    probability_value = F.sigmoid(result).item()\n",
    "    index = 1\n",
    "    if probability_value < decision_threshold:\n",
    "        index = 0\n",
    "\n",
    "    return vectorizer.rating_vocab.lookup_index(index), probability_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The review 'this is a pretty amazing restaurant' is POSITIVE with probability 0.8731887936592102\n"
     ]
    }
   ],
   "source": [
    "test_review = \"this is a pretty amazing restaurant\"\n",
    "\n",
    "prediction, probability = predict_rating(test_review, classifier, vectorizer, decision_threshold=0.5)\n",
    "print(\"The review '{}' is {}\".format(test_review, prediction.upper()), 'with probability', probability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Influential words in positive reviews:\n",
      "--------------------------------------\n",
      "pleasantly\n",
      "chinatown\n",
      "fantastic\n",
      "deliciousness\n",
      "amazed\n",
      "hooked\n",
      "nthank\n",
      "delicious\n",
      "jokes\n",
      "ngreat\n",
      "delightful\n",
      "superb\n",
      "perfection\n",
      "bomb\n",
      "lawn\n",
      "drawback\n",
      "notch\n",
      "mmmm\n",
      "maker\n",
      "frills\n",
      "====\n",
      "\n",
      "\n",
      "\n",
      "Influential words in negative reviews:\n",
      "--------------------------------------\n",
      "worst\n",
      "meh\n",
      "nmaybe\n",
      "slowest\n",
      "mediocre\n",
      "bland\n",
      "awful\n",
      "unacceptable\n",
      "underwhelmed\n",
      "horrible\n",
      "cancelled\n",
      "tasteless\n",
      "rude\n",
      "embarrassing\n",
      "unimpressed\n",
      "blah\n",
      "disgusting\n",
      "terrible\n",
      "horrendous\n",
      "roach\n"
     ]
    }
   ],
   "source": [
    "# This should be fc1 here (input layer) as this is where components\n",
    "# make sense\n",
    "fc1_weights = classifier.fc1.weight.detach()[0]\n",
    "_, indices = torch.sort(fc1_weights, dim=0)\n",
    "indices = indices.numpy().tolist()\n",
    "\n",
    "print(\"Influential words in positive reviews:\")\n",
    "print(\"--------------------------------------\")\n",
    "for i in range(20):\n",
    "    print(vectorizer.review_vocab.lookup_index(indices[i]))\n",
    "    \n",
    "print(\"====\\n\\n\\n\")\n",
    "\n",
    "print(\"Influential words in negative reviews:\")\n",
    "print(\"--------------------------------------\")\n",
    "indices.reverse()\n",
    "for i in range(20):\n",
    "    print(vectorizer.review_vocab.lookup_index(indices[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_to_idx': {'negative': 0, 'positive': 1}, 'idx_to_token': {0: 'negative', 1: 'positive'}}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.rating_vocab.to_dictionary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "print(classifier.fc2.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Influential words in positive reviews:\n",
      "--------------------------------------\n",
      "pleasantly\n",
      "fantastic\n",
      "chinatown\n",
      "delicious\n",
      "deliciousness\n",
      "hooked\n",
      "amazed\n",
      "nthank\n",
      "ngreat\n",
      "jokes\n",
      "bomb\n",
      "superb\n",
      "notch\n",
      "mmmm\n",
      "delightful\n",
      "lawn\n",
      "perfection\n",
      "drawback\n",
      "boba\n",
      "frills\n",
      "Influential words in negative reviews:\n",
      "--------------------------------------\n",
      "worst\n",
      "meh\n",
      "nmaybe\n",
      "slowest\n",
      "mediocre\n",
      "bland\n",
      "awful\n",
      "unacceptable\n",
      "underwhelmed\n",
      "horrible\n",
      "tasteless\n",
      "cancelled\n",
      "rude\n",
      "blah\n",
      "embarrassing\n",
      "disgusting\n",
      "unimpressed\n",
      "terrible\n",
      "unfriendly\n",
      "horrendous\n"
     ]
    }
   ],
   "source": [
    "W1 = classifier.fc1.weight.detach()      \n",
    "W2 = classifier.fc2.weight.detach()      \n",
    "\n",
    "# Effective weights: class × vocab\n",
    "W_eff = W2 @ W1                          \n",
    "\n",
    "topk = torch.topk(W_eff, k=20)\n",
    "print(\"Influential words in positive reviews:\")\n",
    "print(\"--------------------------------------\")\n",
    "for i in topk.indices[0]:\n",
    "        print(vectorizer.review_vocab.lookup_index(i.item()))\n",
    "\n",
    "print(\"Influential words in negative reviews:\")\n",
    "print(\"--------------------------------------\")\n",
    "topk = torch.topk(-W_eff, k=20)\n",
    "for i in topk.indices[0]:\n",
    "        print(vectorizer.review_vocab.lookup_index(i.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
