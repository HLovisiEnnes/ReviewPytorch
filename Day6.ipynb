{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation with autoregessive models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will again use a GRU model on the `surname` dataste, but this time, the task will be generating new surnames (conditioned on the nationalities). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset loading classes will be basically the same as yesterday's, except that now, when vectorizing, we will be encoding both input and output. For vocabulary, we will basically copy the classes from yesterday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    # The class is very similar to the vocabulary class of Day 2,\n",
    "    # except that we don't have an unknown option, which will be \n",
    "    # added to its subclass only\n",
    "    def __init__(self, token_to_idx = None):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx =  token_to_idx\n",
    "        self._idx_to_token = {idx: token\n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return {'token_to_idx': self._token_to_idx, \n",
    "                'idx_to_token': self._idx_to_token}\n",
    "    \n",
    "    def add_token(self, token):\n",
    "        if token in self._token_to_idx:\n",
    "            # If already in the dictionary, do nothing\n",
    "            return self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "            return index\n",
    "    \n",
    "    def add_many(self, tokens):\n",
    "        return list(map(self.add_token, tokens))\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        return self._token_to_idx[token]\n",
    "    \n",
    "    def lookup_index(self, index):\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "    \n",
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx = None, unk_token = '<UNK>',\n",
    "                 maks_token = '<MASK>', begin_seq_token = '<BEGIN>', end_seq_token = '<END>'):\n",
    "        super().__init__(token_to_idx)\n",
    "\n",
    "        self._unk_token = unk_token\n",
    "        self._mask_token = maks_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        # The inverse of ordering below is on purpose, we want <MASK> to be the first id\n",
    "        # so that we can apply the embedding layer without calling an optional argument.\n",
    "        # It is possible not to invert this order, however.\n",
    "        self.mask_idx = self.add_token(self._mask_token)\n",
    "        self.unk_idx = self.add_token(self._unk_token)\n",
    "        self.begin_seq_idx = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_idx = self.add_token(self._end_seq_token)\n",
    "\n",
    "    def to_dict(self):\n",
    "        contents = super().to_dict()\n",
    "        contents.update({'unk_token': self._unk_token,\n",
    "                         'mask_token': self._mask_token,\n",
    "                         'begin_seq_token': self._begin_seq_token,\n",
    "                         'end_seq_token': self._end_seq_token})\n",
    "        return contents\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        if self.unk_idx >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_idx)\n",
    "        else:\n",
    "            return self._token_to_idx[token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us give a look at the vectorizer. The main idea here is that we will create two vectors representing the sequence, from_vector and to_vector. The difference is that, at time t, from_vector[t] is lagging one token with respect to to_vector[t]. In particular, from_vector starts at the sequence 0th input and ends at input -2, whereas to_vector starts at 1st and ends at -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class SunermaneVectorizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def vectorize(self, surname, vector_len = -1):\n",
    "        indices = [self.vocab.begin_seq_idx]\n",
    "        for token in surname:\n",
    "            indices.append(self.vocab.lookup_token(token))\n",
    "        indices.append(self.vocab.end_seq_idx)\n",
    "\n",
    "        if vector_len < 0:\n",
    "            vector_len = len(indices)\n",
    "        \n",
    "        # Input vector\n",
    "        from_vector = np.zeros(vector_len, dtype = np.int64)\n",
    "        from_indices = indices[:-1]\n",
    "        from_vector[:len(from_indices)] = from_indices\n",
    "        from_vector[len(from_indices):] = self.vocab.mask_idx\n",
    "\n",
    "        # Output vector\n",
    "        to_vector = np.zeros(vector_len, dtype = np.int64)\n",
    "        to_indices = indices[1:]\n",
    "        to_vector[:len(to_indices)] = to_indices\n",
    "        to_vector[len(to_indices):] = self.vocab.mask_idx\n",
    "\n",
    "        return from_vector, to_vector\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, data):\n",
    "        voc = SequenceVocabulary()\n",
    "\n",
    "        for _, row in data.iterrows():\n",
    "            for char in row['surname']:\n",
    "                voc.add_token(char)\n",
    "        \n",
    "        return cls(voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we change the SurnameDataset to allow the correct outputs, that is, intead of classes, vectors. Only the getitem method will therefore change significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SurnameDataset(Dataset):\n",
    "    def __init__(self, sunrmane_df, vectorizer):\n",
    "        self.surname_df = sunrmane_df\n",
    "        self.vectorizer = vectorizer\n",
    "        \n",
    "        self._max_sequence_len = max(map(len, self.surname_df)) + 2\n",
    "\n",
    "        self.train_df = self.surname_df[self.surname_df['split'] == 'train']\n",
    "        self.len_train = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.surname_df[self.surname_df['split'] == 'val']\n",
    "        self.len_val = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.surname_df[self.surname_df['split'] == 'test']\n",
    "        self.len_test = len(self.test_df)\n",
    "\n",
    "        self._lookup_table = {'train': (self.train_df, self.len_train),\n",
    "                              'val': (self.val_df, self.len_val),\n",
    "                              'test': (self.test_df, self.len_test)}\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "    def set_split(self, split = 'train'):\n",
    "        self._target_split, self._len_split = self._lookup_table[split]\n",
    "    \n",
    "    @classmethod\n",
    "    def load_dataset(cls, path):\n",
    "        surnames = pd.read_csv(path)\n",
    "        return cls(surnames, SunermaneVectorizer.from_dataframe(surnames))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._len_split\n",
    "\n",
    "    def __getitem__(self, row_index):\n",
    "        row = self._target_split.iloc[row_index]\n",
    "        from_vector, to_vector  = self.vectorizer.vectorize(row['surname'], self._max_sequence_len)\n",
    "        return {\n",
    "            'x_data': from_vector,\n",
    "            'y_data': to_vector,\n",
    "            'class_index': row['nationality_index']}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self)//batch_size\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle = True,\n",
    "                     drop_last = True, device = 'cuda'): \n",
    "    # drop_last decides whether to drop the last batch if len(daset) % batchsize != 0\n",
    "    dataloader = DataLoader(dataset = dataset, batch_size = batch_size,\n",
    "                            shuffle = shuffle, drop_last = drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, _ in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check if everything is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 4 5 6 7 8 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[4 5 6 7 8 3 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "surnames = SurnameDataset.load_dataset('surnames_with_splits.csv')\n",
    "print(surnames[0]['x_data'])\n",
    "print(surnames[0]['y_data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that, indeed, `x_data` is 1-lag behind `y_data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will be only slightly different from last GRU model, only changing the head (no classification needed), include an embedding of the nationality, and add some dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class SurnameGenerationModel(nn.Module):\n",
    "    def __init__(self, char_embedding_size, char_vocab_size, num_nationalities,\n",
    "                 rnn_hidden_size, batch_first = True, padding_idx = 0, dropout_p = 0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.char_emb = nn.Embedding(num_embeddings = char_vocab_size,\n",
    "                                     embedding_dim = char_embedding_size,\n",
    "                                     padding_idx = padding_idx)\n",
    "\n",
    "        self.nation_emb = nn.Embedding(num_embeddings = num_nationalities,\n",
    "                                       embedding_dim = rnn_hidden_size)\n",
    "\n",
    "        self.rnn = nn.GRU(input_size = char_embedding_size, \n",
    "                          hidden_size = rnn_hidden_size,\n",
    "                          batch_first = batch_first)\n",
    "        \n",
    "        self.fc = nn.Linear(in_features = rnn_hidden_size, \n",
    "                            out_features = char_vocab_size)\n",
    "        \n",
    "        self._dropout_p = dropout_p\n",
    "\n",
    "    def forward(self, x_in, nationality_index, apply_softmax = False):\n",
    "        x_embedded = self.char_emb(x_in)\n",
    "        \n",
    "        nationality_embedded = self.nation_emb(nationality_index).unsqueeze(0)\n",
    "\n",
    "        # We use the nationality embedding as a first hidden state\n",
    "        # Recall that y_out has all hidden states, not only the last one\n",
    "        y_out, _ = self.rnn(x_embedded, nationality_embedded)\n",
    "\n",
    "        batch_size, seq_size, feat_size = y_out.shape\n",
    "        y_out = y_out.contiguous().view(batch_size * seq_size, feat_size)\n",
    "\n",
    "        y_out = self.fc(F.dropout(y_out, p = self._dropout_p))\n",
    "                         \n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=1)\n",
    "\n",
    "        new_feat_size = y_out.shape[-1]\n",
    "        y_out = y_out.view(batch_size, seq_size, new_feat_size)\n",
    "            \n",
    "        return y_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we modularized the code (always modularize code!), training basically does not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience = 5, min_delta = 0, save = None):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.save = save\n",
    "        self.best_loss = np.inf\n",
    "        self.patience_counter = 0\n",
    "        self.flag = False\n",
    "    \n",
    "    def __call__(self, val_loss, model = None):\n",
    "        # If the validation loss improved, we basically \n",
    "        # update the new best lost and save the model\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.patience_counter = 0\n",
    "        \n",
    "            if (self.save) and (model is not None):\n",
    "                torch.save(model.state_dict(), self.save)\n",
    "        \n",
    "        else:\n",
    "            self.patience_counter += 1\n",
    "            if self.patience_counter > self.patience:\n",
    "                self.flag = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss will necessairly change a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sizes(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Normalize tensor sizes to meet what is expected by the loss function.\n",
    "    \"\"\"\n",
    "    if len(y_pred.size()) == 3:\n",
    "        y_pred = y_pred.contiguous().view(-1, y_pred.size(2))\n",
    "    if len(y_true.size()) == 2:\n",
    "        y_true = y_true.contiguous().view(-1)\n",
    "    return y_pred, y_true\n",
    "\n",
    "\n",
    "\n",
    "def compute_accuracy(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    \n",
    "    correct_indices = torch.eq(y_pred_indices, y_true).float()\n",
    "    # Only pick the indices that have been not masked to compute the\n",
    "    # error, otherwise there would be a sequence length bias\n",
    "    valid_indices = torch.ne(y_true, mask_index).float()\n",
    "    \n",
    "    n_correct = (correct_indices * valid_indices).sum().item()\n",
    "    n_valid = valid_indices.sum().item()\n",
    "\n",
    "    return n_correct / n_valid * 100\n",
    "\n",
    "def sequence_loss(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "    return F.cross_entropy(y_pred, y_true, ignore_index=mask_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Validation loss 3.4284615119298296\n",
      "Validation accuracy:  11.703848558619095\n",
      "Epoch:  1\n",
      "Validation loss 3.1108711759249372\n",
      "Validation accuracy:  16.954573372335798\n",
      "Epoch:  2\n",
      "Validation loss 2.938374082247416\n",
      "Validation accuracy:  18.887165055162278\n",
      "Epoch:  3\n",
      "Validation loss 2.822935819625855\n",
      "Validation accuracy:  20.920857264460672\n",
      "Epoch:  4\n",
      "Validation loss 2.749722341696421\n",
      "Validation accuracy:  22.260679767018736\n",
      "Epoch:  5\n",
      "Validation loss 2.699275175730387\n",
      "Validation accuracy:  22.932706815493596\n",
      "Epoch:  6\n",
      "Validation loss 2.6611728469530744\n",
      "Validation accuracy:  23.667307312633064\n",
      "Epoch:  7\n",
      "Validation loss 2.6381132801373797\n",
      "Validation accuracy:  24.035927241641417\n",
      "Epoch:  8\n",
      "Validation loss 2.6024572451909385\n",
      "Validation accuracy:  25.134773236507908\n",
      "Epoch:  9\n",
      "Validation loss 2.5872876246770224\n",
      "Validation accuracy:  25.13582065617907\n",
      "Epoch:  10\n",
      "Validation loss 2.58618156115214\n",
      "Validation accuracy:  25.47710355463245\n",
      "Epoch:  11\n",
      "Validation loss 2.5856249729792276\n",
      "Validation accuracy:  25.063223490215993\n",
      "Epoch:  12\n",
      "Validation loss 2.581326444943746\n",
      "Validation accuracy:  25.479407587632963\n",
      "Epoch:  13\n",
      "Validation loss 2.5839806397755942\n",
      "Validation accuracy:  25.458962017972805\n",
      "Epoch:  14\n",
      "Validation loss 2.577988306681315\n",
      "Validation accuracy:  25.641881054355636\n",
      "Epoch:  15\n",
      "Validation loss 2.5795140067736306\n",
      "Validation accuracy:  25.275389707135496\n",
      "Epoch:  16\n",
      "Validation loss 2.577389339605967\n",
      "Validation accuracy:  25.579550805579967\n",
      "Epoch:  17\n",
      "Validation loss 2.5765164097150173\n",
      "Validation accuracy:  25.399460544525805\n",
      "Epoch:  18\n",
      "Validation loss 2.576398233572642\n",
      "Validation accuracy:  25.635946672435544\n",
      "Epoch:  19\n",
      "Validation loss 2.5817851026852923\n",
      "Validation accuracy:  25.508055209757288\n",
      "Epoch:  20\n",
      "Validation loss 2.5717190901438394\n",
      "Validation accuracy:  25.458367647869665\n",
      "Epoch:  21\n",
      "Validation loss 2.5704481403032937\n",
      "Validation accuracy:  25.765908074376974\n",
      "Epoch:  22\n",
      "Validation loss 2.571868141492208\n",
      "Validation accuracy:  25.996196259946885\n",
      "Epoch:  23\n",
      "Validation loss 2.5743726094563804\n",
      "Validation accuracy:  25.74350545634652\n",
      "Epoch:  24\n",
      "Validation loss 2.568421999613444\n",
      "Validation accuracy:  25.718318429529553\n",
      "Epoch:  25\n",
      "Validation loss 2.561912715435028\n",
      "Validation accuracy:  25.84680383723672\n",
      "Epoch:  26\n",
      "Validation loss 2.571650207042694\n",
      "Validation accuracy:  25.814180649186415\n",
      "Epoch:  27\n",
      "Validation loss 2.5722778240839643\n",
      "Validation accuracy:  25.821764937056876\n",
      "Epoch:  28\n",
      "Validation loss 2.5717649459838876\n",
      "Validation accuracy:  25.400302095610474\n",
      "Epoch:  29\n",
      "Validation loss 2.565502067406972\n",
      "Validation accuracy:  25.500789731564893\n",
      "Epoch:  30\n",
      "Validation loss 2.5687546133995056\n",
      "Validation accuracy:  25.4426268502529\n",
      "Early stopped at epoch: 31\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Dataset\n",
    "vectorizer = surnames.vectorizer\n",
    "mask_index = vectorizer.vocab.mask_idx\n",
    "\n",
    "\n",
    "# Model hyperparameters\n",
    "char_embedding_size = 32\n",
    "rnn_hidden_size = 32\n",
    "\n",
    "# Training hyperparameters\n",
    "lr = 1e-3\n",
    "n_epochs = 100\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "# Allows for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Model\n",
    "classifier = SurnameGenerationModel(char_embedding_size = char_embedding_size,\n",
    "                               char_vocab_size = len(vectorizer.vocab),\n",
    "                               num_nationalities = len(surnames.train_df['nationality_index'].unique()),\n",
    "                               rnn_hidden_size= rnn_hidden_size,\n",
    "                               padding_idx = mask_index,\n",
    "                               dropout_p = 0.5)\n",
    "classifier.to(device)\n",
    "\n",
    "\n",
    "# Optimization\n",
    "optimizer = optim.Adam(classifier.parameters(), lr = lr)\n",
    "scheduler = StepLR(optimizer, step_size = 10, gamma = 0.1)\n",
    "early_stopping = EarlyStopping(min_delta = 0, save = 'best_gru_generation_model.pt')\n",
    "\n",
    "for epoch in range(n_epochs): \n",
    "    surnames.set_split('train')\n",
    "    batch_generator = generate_batches(surnames, batch_size = batch_size, device = device)\n",
    "    classifier.train()\n",
    "\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # 1. Zero the gradient\n",
    "        classifier.zero_grad()\n",
    "\n",
    "        # 2. Prediction\n",
    "        y_pred = classifier(x_in = batch_dict['x_data'],\n",
    "                            nationality_index = batch_dict['class_index'])\n",
    "\n",
    "        # 3. Compute loss\n",
    "        loss = sequence_loss(y_pred, batch_dict['y_data'], mask_index)\n",
    "\n",
    "\n",
    "        # 4. Backpropagate\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Optimize\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict['y_data'], mask_index)\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "    # Evaluation part, we don't want paramereres to change\n",
    "    classifier.eval()\n",
    "    surnames.set_split('val')\n",
    "\n",
    "    batch_generator = generate_batches(surnames, batch_size = batch_size, device = device)\n",
    "    \n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    classifier.eval()\n",
    "\n",
    "\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        y_pred = classifier(x_in = batch_dict['x_data'], \n",
    "                            nationality_index = batch_dict['class_index'])\n",
    "\n",
    "        loss = sequence_loss(y_pred, batch_dict['y_data'], mask_index)\n",
    "\n",
    "        running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict['y_data'], mask_index)\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "    # Updates early stop\n",
    "    early_stopping(running_loss, classifier)\n",
    "    if early_stopping.flag:\n",
    "        print('Early stopped at epoch:', epoch)\n",
    "        break\n",
    "\n",
    "    # Updates scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    print('Epoch: ', epoch)\n",
    "    print('Validation loss', running_loss)\n",
    "    print('Validation accuracy: ', running_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss 2.5740342140197754\n",
      "Validation accuracy:  25.85422707320446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7165/2549440820.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  classifier.load_state_dict(torch.load(\"best_gru_generation_model.pt\"))\n"
     ]
    }
   ],
   "source": [
    "classifier.load_state_dict(torch.load(\"best_gru_generation_model.pt\"))\n",
    "classifier.eval()\n",
    "\n",
    "# Evaluation part, we don't want paramereres to change\n",
    "classifier.eval()\n",
    "surnames.set_split('test')\n",
    "\n",
    "batch_generator = generate_batches(surnames, batch_size = batch_size, device = device)\n",
    "\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        y_pred = classifier(x_in = batch_dict['x_data'], \n",
    "                            nationality_index = batch_dict['class_index'])\n",
    "\n",
    "        loss = sequence_loss(y_pred, batch_dict['y_data'], mask_index)\n",
    "\n",
    "        running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict['y_data'], mask_index)\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "print('Validation loss', running_loss)\n",
    "print('Validation accuracy: ', running_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very bad, but the one obtained by the book is not much better (27%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now implement here sample decoding to use in the model. We will be using the temperature transformation to control the degree of hallucination of the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_model(model, vectorizer, nationalities, sample_size = 20,\n",
    "                      temperature = 1.0):\n",
    "    # We can sample more than a sample each time\n",
    "    num_samples = len(nationalities)\n",
    "    # We will have a batch of num_samples, each starting \n",
    "    # with begin_seq_idx\n",
    "    begin_seq_idx = [vectorizer.vocab.begin_seq_idx\n",
    "                     for _ in range(num_samples)]\n",
    "    begin_seq_idx = torch.tensor(begin_seq_idx, \n",
    "                                 dtype = torch.int64).unsqueeze(dim = 1)\n",
    "    # This will be appended at each new time step\n",
    "    indices = [begin_seq_idx]\n",
    "    nationality_idx = torch.tensor(nationalities, \n",
    "                                   dtype = torch.int64)\n",
    "    # We embed the nationality as the 0th hidden state of the model\n",
    "    h_t = model.nation_emb(nationality_idx).unsqueeze(0)\n",
    "    \n",
    "\n",
    "\n",
    "    for time_step in range(sample_size):\n",
    "        # We will basically reconstruct the model here\n",
    "        # so that we can allow predictions of one time step\n",
    "        # to serve as input of the next\n",
    "        x_t = indices[time_step]\n",
    "        x_embd_t = model.char_emb(x_t)\n",
    "        rnn_out , h_t = model.rnn(x_embd_t, h_t)\n",
    "        prediction_vector = model.fc(rnn_out.squeeze(dim = 1))\n",
    "        probability_vector = F.softmax(prediction_vector / temperature, dim=1)\n",
    "        # This samples according to the monomial given by the probability vector\n",
    "        indices.append(torch.multinomial(probability_vector, num_samples=1))\n",
    "    \n",
    "    # We permute here because, as of now, it is of form [seq, batch]\n",
    "    \n",
    "    indices = torch.stack(indices).squeeze()\n",
    "    indices = indices.view(indices.size(0),-1).permute(1, 0)\n",
    "    return indices\n",
    "\n",
    "def decode_samples(sampled_indices, vectorizer):\n",
    "    decoded_surnames = []\n",
    "    vocab = vectorizer.vocab\n",
    "    \n",
    "    for sample_idx in range(sampled_indices.shape[0]):\n",
    "        surname = ''\n",
    "        for time_step in range(sampled_indices.shape[1]):\n",
    "            sampled_item = sampled_indices[sample_idx, time_step].item()\n",
    "            if sampled_item == vocab.begin_seq_idx:\n",
    "                continue\n",
    "            elif sampled_item == vocab.end_seq_idx:\n",
    "                break\n",
    "            else:\n",
    "                surname += vocab.lookup_index(sampled_item)\n",
    "        decoded_surnames.append(surname)\n",
    "    return decoded_surnames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tons',\n",
       " 'Nater',\n",
       " 'Solun',\n",
       " 'Ran',\n",
       " 'Sarger',\n",
       " 'Harir',\n",
       " 'Parna',\n",
       " 'Belso',\n",
       " 'Narla',\n",
       " 'Hashan',\n",
       " 'Bertel',\n",
       " 'Care',\n",
       " 'Mare',\n",
       " 'Selhi',\n",
       " 'Samer',\n",
       " 'Mannes',\n",
       " 'Wetto',\n",
       " 'Sher',\n",
       " 'Nadel',\n",
       " 'Serer']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = classifier.cpu()\n",
    "# This makes a dictionary where nationality is written as an index\n",
    "nationality_to_idx = surnames.train_df.set_index(\"nationality\")[\"nationality_index\"].to_dict()\n",
    "decode_samples(sample_from_model(classifier, vectorizer, [nationality_to_idx['German']]*20,  temperature = 0.4), vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not very good, but well, expect for such a low accuracy model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
