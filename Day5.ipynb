{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoregressive models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will implement two very simple autoregressive models, namely Elman and GRUs. We will apply them to the surname dataset for both classification of the nationality. Tomorrow, this same code will be used for geenrating new surnames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by exploring the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameDataset(Dataset):\n",
    "    def __init__(self, sunrmane_df, vectorizer):\n",
    "        self.surname_df = sunrmane_df\n",
    "        self.vectorizer = vectorizer\n",
    "        \n",
    "        # I *really* liked this syntax solution of the original implementation\n",
    "        self._max_sequence_len = max(map(len, self.surname_df)) + 2\n",
    "\n",
    "        self.train_df = self.surname_df[self.surname_df['split'] == 'train']\n",
    "        self.len_train = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.surname_df[self.surname_df['split'] == 'val']\n",
    "        self.len_val = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.surname_df[self.surname_df['split'] == 'test']\n",
    "        self.len_test = len(self.test_df)\n",
    "\n",
    "        self._lookup_table = {'train': (self.train_df, self.len_train),\n",
    "                              'val': (self.val_df, self.len_val),\n",
    "                              'test': (self.test_df, self.len_test)}\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "        # Get class weights, we sort them by the index and only get the values\n",
    "        value_counts = self.train_df['nationality_index'].value_counts().sort_index().values\n",
    "        self.class_weights = 1.0 / torch.tensor(value_counts, dtype = torch.float32)\n",
    "\n",
    "    def set_split(self, split = 'train'):\n",
    "        self._target_split, self._len_split = self._lookup_table[split]\n",
    "    \n",
    "    @classmethod\n",
    "    def load_dataset(cls, path):\n",
    "        surnames = pd.read_csv(path)\n",
    "        return cls(surnames, SunermaneVectorizer.from_dataframe(surnames))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._len_split\n",
    "\n",
    "    def __getitem__(self, row_index):\n",
    "        row = self._target_split.iloc[row_index]\n",
    "        surname_vector, vec_len = self.vectorizer.vectorize(row['surname'], self._max_sequence_len)\n",
    "        return {\n",
    "            'x_data': surname_vector,\n",
    "            'y_data': row['nationality_index'],\n",
    "            'x_len': vec_len}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self)//batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are working at the character level, a TF-IDF approach chould make sense for the classication task, but, for the generation task, it would be unecessarily complicated. Therefore, we shall use the same one-hot encoding for both tasks. This time, we also want to use a sequence vocabulary class in which we will add the special tokens (UNK, MASK, BEGIN, END) to the tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    # The class is very similar to the vocabulary class of Day 2,\n",
    "    # except that we don't have an unknown option, which will be \n",
    "    # added to its subclass only\n",
    "    def __init__(self, token_to_idx = None):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx =  token_to_idx\n",
    "        self._idx_to_token = {idx: token\n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return {'token_to_idx': self._token_to_idx, \n",
    "                'idx_to_token': self._idx_to_token}\n",
    "    \n",
    "    def add_token(self, token):\n",
    "        if token in self._token_to_idx:\n",
    "            # If already in the dictionary, do nothing\n",
    "            return self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "            return index\n",
    "    \n",
    "    def add_many(self, tokens):\n",
    "        return list(map(self.add_token, tokens))\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        return self._token_to_idx[token]\n",
    "    \n",
    "    def lookup_index(self, index):\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx = None, unk_token = '<UNK>',\n",
    "                 maks_token = '<MASK>', begin_seq_token = '<BEGIN>', end_seq_token = '<END>'):\n",
    "        super().__init__(token_to_idx)\n",
    "\n",
    "        self._unk_token = unk_token\n",
    "        self._mask_token = maks_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        # The inverse of ordering below is on purpose, we want <MASK> to be the first id\n",
    "        # so that we can apply the embedding layer without calling an optional argument.\n",
    "        # It is possible not to invert this order, however.\n",
    "        self.mask_idx = self.add_token(self._mask_token)\n",
    "        self.unk_idx = self.add_token(self._unk_token)\n",
    "        self.begin_seq_idx = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_idx = self.add_token(self._end_seq_token)\n",
    "\n",
    "    def to_dict(self):\n",
    "        contents = super().to_dict()\n",
    "        contents.update({'unk_token': self._unk_token,\n",
    "                         'mask_token': self._mask_token,\n",
    "                         'begin_seq_token': self._begin_seq_token,\n",
    "                         'end_seq_token': self._end_seq_token})\n",
    "        return contents\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        if self.unk_idx >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_idx)\n",
    "        else:\n",
    "            return self._token_to_idx[token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the vectorizer. It is basically the same as last time, but only now working at the character and not at the word level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class SunermaneVectorizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def vectorize(self, surname, vector_len = -1):\n",
    "        indices = [self.vocab.begin_seq_idx]\n",
    "        for token in surname:\n",
    "            indices.append(self.vocab.lookup_token(token))\n",
    "        indices.append(self.vocab.end_seq_idx)\n",
    "\n",
    "        # We do not truncate the indices if this is -1\n",
    "        if vector_len < 0:\n",
    "            vector_len = len(indices)\n",
    "        \n",
    "        # Now we do the one-hot encoding\n",
    "        out_vector = np.zeros(vector_len, dtype = np.int64)\n",
    "        # We assign the index to the tokens that do show up in the surname and\n",
    "        # mask the part of the vector longer than the input sequence\n",
    "        out_vector[:len(indices)] = indices\n",
    "        out_vector[len(indices):] = self.vocab.mask_idx\n",
    "\n",
    "        return out_vector, len(indices)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, data):\n",
    "        voc = SequenceVocabulary()\n",
    "\n",
    "        for _, row in data.iterrows():\n",
    "            for char in row['surname']:\n",
    "                voc.add_token(char)\n",
    "        \n",
    "        return cls(voc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<BEGIN>Totah<END>'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surnames =  Surnamesurnames.load_surnames('surnames_with_splits.csv')\n",
    "vectorizer = surnames.vectorizer\n",
    "# Let us check the first surname\n",
    "\"\".join(list(map(surnames.vectorizer.vocab.lookup_index,surnames[0]['x_data']))[:surnames[0]['x_len']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we implement the batch generator function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle = True,\n",
    "                     drop_last = True, device = 'cuda'): \n",
    "    # drop_last decides whether to drop the last batch if len(daset) % batchsize != 0\n",
    "    dataloader = DataLoader(dataset = dataset, batch_size = batch_size,\n",
    "                            shuffle = shuffle, drop_last = drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, _ in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first implement the Elman RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ElmanRNN(nn.Module):\n",
    "    '''\n",
    "    This will correspond to only a layer of our architecture.\n",
    "    '''\n",
    "    def __init__(self, input_size, hidden_size, batch_first = False):\n",
    "        # If batch_first is True, the first dimension is the batch\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn_cell =  nn.RNNCell(input_size, hidden_size)\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "    def _initial_hidden(self, batch_size):\n",
    "        # The initial hidden state is only the zero tensor\n",
    "        # Notice that this is the hidden state of only *one* token\n",
    "        return torch.zeros((batch_size, self.hidden_size))\n",
    "    \n",
    "    def forward(self, x_in, initial_hidden = None):\n",
    "        '''\n",
    "        The otuput will be of rank (batch, seq_size, hidden_state) if\n",
    "        batch_first, otherwise it will be (seq_size, batch, hidden_state).\n",
    "        '''\n",
    "        if self.batch_first:\n",
    "            batch_size, seq_size, feat_size = x_in.size()\n",
    "            # In RNNs, we assume that the batch is the second dimension\n",
    "            x_in =  x_in.permute(1, 0, 2)\n",
    "        else:\n",
    "            seq_size, batch_size, feat_size = x_in.size()\n",
    "\n",
    "        hiddens = []\n",
    "\n",
    "        if initial_hidden is None:\n",
    "            initial_hidden = self._initial_hidden(batch_size)\n",
    "            # Makes the same device as the x_in\n",
    "            initial_hidden = initial_hidden.to(x_in.device)\n",
    "\n",
    "        # Hidden at time -1, this is not added to the sequence\n",
    "        hidden_t = initial_hidden\n",
    "                    \n",
    "        for t in range(seq_size):\n",
    "            # Notice that here we unroll the RNN cell\n",
    "            hidden_t = self.rnn_cell(x_in[t], hidden_t)\n",
    "            hiddens.append(hidden_t)\n",
    "\n",
    "        # Stack the whole list of tensors along dimension 0, this case,\n",
    "        # sequence dimension (recall we permuted if batch first)\n",
    "        hiddens = torch.stack(hiddens)\n",
    "\n",
    "        if self.batch_first:\n",
    "            # Repermute to have batch first\n",
    "            hiddens = hiddens.permute(1, 0, 2)\n",
    "\n",
    "        return hiddens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the nice things about this example is that the RNN part is only but a single cell, which can be thought of as a layer. We can add it to a more complicated architecture. The first step in thgat direction is to make a function that gives us the last hidden state of a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def column_gather(y_out, x_lengths):\n",
    "    '''\n",
    "    This function is for getting the last hidden state of each input in a \n",
    "    batch.\n",
    "\n",
    "    Args:\n",
    "        y_out: (batch, sequence, feature)\n",
    "        x_lengths: (batch,)\n",
    "\n",
    "    Returns:\n",
    "        (batch, feature)\n",
    "    '''\n",
    "    # detatch is used to have the tensor not part of the computational graph,\n",
    "    # so no gradients are take. Same as no_grad, but instead of holding\n",
    "    # within an environment, holds for the full tensor.\n",
    "    # The -1 is because, as usual, we start counting from 0\n",
    "    x_lengths = x_lengths.long().detach().cpu().numpy() - 1\n",
    "\n",
    "    out = []\n",
    "    for batch_index, column_index in enumerate(x_lengths):\n",
    "        out.append(y_out[batch_index, column_index])\n",
    "\n",
    "    return torch.stack(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the whole model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SurnameClassifier(nn.Module):\n",
    "    def __init__(self, embediing_size, num_embeddings, num_classes,\n",
    "                 rnn_hidden_size, batch_first = True, padding_idx = 0):\n",
    "        '''\n",
    "        num_embeddings: Number of characters to embedding (i.e., vocab size).\n",
    "        padding_idx: This index will be ignored by the later layers of the model\n",
    "        as it only indicates padding of the vectors. In our case, we chose the\n",
    "        padding token, <MASK>, to have index 0\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(num_embeddings = num_embeddings, \n",
    "                                embedding_dim = embediing_size,\n",
    "                                padding_idx = padding_idx)\n",
    "        self.rnn = ElmanRNN(input_size = embediing_size,\n",
    "                             hidden_size = rnn_hidden_size,\n",
    "                             batch_first = batch_first)\n",
    "        self.fc1 = nn.Linear(in_features = rnn_hidden_size,\n",
    "                         out_features = rnn_hidden_size)\n",
    "        self.fc2 = nn.Linear(in_features = rnn_hidden_size,\n",
    "                          out_features = num_classes)\n",
    "        \n",
    "    def forward(self, x_in, x_lens =  None, apply_softmax = False):\n",
    "        '''\n",
    "        x_in: should be of shape (batch, input_dim)\n",
    "        x_lens: the length of each sequence of the batch, used to \n",
    "            find the last hidden state\n",
    "        Returns:\n",
    "            tensor of shape (batch, output_dim) \n",
    "        '''\n",
    "        x_embedded = self.emb(x_in)\n",
    "        y_out = self.rnn(x_embedded)\n",
    "\n",
    "        if x_lens is not None:\n",
    "            y_out = column_gather(y_out, x_lens)\n",
    "        else:\n",
    "            # If no lens are proviced, we do not truncate\n",
    "            # the sequence, we take for last hidden state\n",
    "            # the actual last component of the vector\n",
    "            y_out = y_out[:, -1, :]\n",
    "\n",
    "        # We use a 2-layer MLP for classification\n",
    "        y_out = F.relu(self.fc1(F.dropout(y_out, 0.5)))\n",
    "        y_out = self.fc2(F.dropout(y_out, 0.5))\n",
    "\n",
    "        # Again, apply softmax only at eval, so to avoid numeric error\n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=1)\n",
    "\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do training. We start implementing early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience = 5, min_delta = 0, save = None):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.save = save\n",
    "        self.best_loss = np.inf\n",
    "        self.patience_counter = 0\n",
    "        self.flag = False\n",
    "    \n",
    "    def __call__(self, val_loss, model = None):\n",
    "        # If the validation loss improved, we basically \n",
    "        # update the new best lost and save the model\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.patience_counter = 0\n",
    "        \n",
    "            if (self.save) and (model is not None):\n",
    "                torch.save(model.state_dict(), self.save)\n",
    "        \n",
    "        else:\n",
    "            self.patience_counter += 1\n",
    "            if self.patience_counter > self.patience:\n",
    "                self.flag = True\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop will change very little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Training loss 2.840860372781754\n",
      "Validation loss 2.7304716682434083\n",
      "Validation accuracy:  0.19124999999999998\n",
      "Epoch:  1\n",
      "Training loss 2.5979037622610734\n",
      "Validation loss 2.501214942932129\n",
      "Validation accuracy:  0.25125\n",
      "Epoch:  2\n",
      "Training loss 2.380781137943268\n",
      "Validation loss 2.3822636556625367\n",
      "Validation accuracy:  0.26749999999999996\n",
      "Epoch:  3\n",
      "Training loss 2.208893961707752\n",
      "Validation loss 2.2003297758102422\n",
      "Validation accuracy:  0.303125\n",
      "Epoch:  4\n",
      "Training loss 2.078051430980366\n",
      "Validation loss 2.119227809906005\n",
      "Validation accuracy:  0.2768749999999999\n",
      "Epoch:  5\n",
      "Training loss 1.9981919825077052\n",
      "Validation loss 2.118076333999634\n",
      "Validation accuracy:  0.31062500000000004\n",
      "Epoch:  6\n",
      "Training loss 1.9388802587985998\n",
      "Validation loss 2.098391551971435\n",
      "Validation accuracy:  0.31625\n",
      "Epoch:  7\n",
      "Training loss 1.8783305386702216\n",
      "Validation loss 1.9897519588470458\n",
      "Validation accuracy:  0.34624999999999995\n",
      "Epoch:  8\n",
      "Training loss 1.847406616806984\n",
      "Validation loss 1.9806051063537595\n",
      "Validation accuracy:  0.3400000000000001\n",
      "Epoch:  9\n",
      "Training loss 1.7910289525985719\n",
      "Validation loss 1.960679364204407\n",
      "Validation accuracy:  0.330625\n",
      "Epoch:  10\n",
      "Training loss 1.754825032750765\n",
      "Validation loss 1.91231981754303\n",
      "Validation accuracy:  0.34437499999999993\n",
      "Epoch:  11\n",
      "Training loss 1.7237070153156917\n",
      "Validation loss 1.9297404861450198\n",
      "Validation accuracy:  0.3575\n",
      "Epoch:  12\n",
      "Training loss 1.7224033286174139\n",
      "Validation loss 2.008180551528931\n",
      "Validation accuracy:  0.3631249999999999\n",
      "Epoch:  13\n",
      "Training loss 1.6950521787007649\n",
      "Validation loss 1.904339022636414\n",
      "Validation accuracy:  0.3706250000000001\n",
      "Epoch:  14\n",
      "Training loss 1.703270823756854\n",
      "Validation loss 1.9549523925781251\n",
      "Validation accuracy:  0.3631249999999999\n",
      "Epoch:  15\n",
      "Training loss 1.6734410852193835\n",
      "Validation loss 1.8838145351409912\n",
      "Validation accuracy:  0.379375\n",
      "Epoch:  16\n",
      "Training loss 1.671470194061597\n",
      "Validation loss 1.9089800786972047\n",
      "Validation accuracy:  0.370625\n",
      "Epoch:  17\n",
      "Training loss 1.648411320646604\n",
      "Validation loss 1.943622388839722\n",
      "Validation accuracy:  0.3637500000000001\n",
      "Epoch:  18\n",
      "Training loss 1.652998752395312\n",
      "Validation loss 1.8827373790740969\n",
      "Validation accuracy:  0.3825\n",
      "Epoch:  19\n",
      "Training loss 1.63127964536349\n",
      "Validation loss 1.912418122291565\n",
      "Validation accuracy:  0.37937499999999996\n",
      "Epoch:  20\n",
      "Training loss 1.62752565741539\n",
      "Validation loss 1.9060837793350218\n",
      "Validation accuracy:  0.38625\n",
      "Epoch:  21\n",
      "Training loss 1.6367664009332659\n",
      "Validation loss 1.9591644954681398\n",
      "Validation accuracy:  0.38625\n",
      "Epoch:  22\n",
      "Training loss 1.6422478834788008\n",
      "Validation loss 1.9567530155181887\n",
      "Validation accuracy:  0.39999999999999997\n",
      "Epoch:  23\n",
      "Training loss 1.6110455006361009\n",
      "Validation loss 1.9978232240676879\n",
      "Validation accuracy:  0.3975\n",
      "Early stopped at epoch: 24\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "lr = 1e-3\n",
    "hidden_dimension = 5\n",
    "n_epochs = 50\n",
    "loss_func = nn.CrossEntropyLoss(surnames.class_weights)\n",
    "loss_func.to(device)\n",
    "batch_size = 64\n",
    "# Allows for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "embediing_size = 100\n",
    "num_embeddings = len(vectorizer.vocab)\n",
    "num_classes = surnames.class_weights.size(0)\n",
    "rnn_hidden_size = 64\n",
    "\n",
    "classifier = SurnameClassifier(embediing_size, num_embeddings, num_classes,\n",
    "                 rnn_hidden_size, batch_first = True, padding_idx = 0)\n",
    "classifier.to(device)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr = lr)\n",
    "# Multiplies the learning rate by `gamma` every `step_size` epoch\n",
    "scheduler = StepLR(optimizer, step_size = 10, gamma = 0.1)\n",
    "\n",
    "early_stopping = EarlyStopping(min_delta = 0, save = 'best_rnn_model.pt')\n",
    "\n",
    "for epoch in range(n_epochs): \n",
    "    surnames.set_split('train')\n",
    "    batch_generator = generate_batches(surnames, batch_size = batch_size, device = device)\n",
    "    running_loss = 0.\n",
    "    classifier.train()\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # 1. Zero the gradient\n",
    "        classifier.zero_grad()\n",
    "\n",
    "        # 2. Prediction\n",
    "        y_pred = classifier(x_in = batch_dict['x_data'],\n",
    "                            x_lens = batch_dict['x_len'])\n",
    "\n",
    "        # 3. Compute loss\n",
    "        loss = loss_func(y_pred, batch_dict['y_data'])\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "\n",
    "        # 4. Backpropagate\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Optimize\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluation part, we don't want paramereres to change\n",
    "    classifier.eval()\n",
    "    surnames.set_split('val')\n",
    "\n",
    "    batch_generator = generate_batches(surnames, batch_size = batch_size, device = device)\n",
    "    \n",
    "    val_loss = 0.\n",
    "    val_acc = 0.\n",
    "    classifier.eval()\n",
    "\n",
    "    \n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # compute the output\n",
    "        y_pred = classifier(x_in = batch_dict['x_data'], \n",
    "                            x_lens = batch_dict['x_len'])\n",
    "\n",
    "        # step 3. compute the loss\n",
    "        loss = loss_func(y_pred, batch_dict['y_data'])\n",
    "        val_loss += (loss.item() - val_loss) / (batch_index + 1)\n",
    "\n",
    "        # compute the accuracy\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict['y_data'])\n",
    "        val_acc += (acc_t - val_acc) / (batch_index + 1)\n",
    "\n",
    "    # Updates early stop\n",
    "    early_stopping(val_loss, classifier)\n",
    "    if early_stopping.flag:\n",
    "        print('Early stopped at epoch:', epoch)\n",
    "        break\n",
    "\n",
    "    # Updates scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    print('Epoch: ', epoch)\n",
    "    print('Training loss', running_loss)\n",
    "    print('Validation loss', val_loss)\n",
    "    print('Validation accuracy: ', val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss 1.8877126789093017\n",
      "Validation accuracy:  0.3793750000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7532/1275012218.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  classifier.load_state_dict(torch.load(\"best_rnn_model.pt\"))\n"
     ]
    }
   ],
   "source": [
    "classifier.load_state_dict(torch.load(\"best_rnn_model.pt\"))\n",
    "classifier.eval()\n",
    "\n",
    "# Evaluation part, we don't want paramereres to change\n",
    "classifier.eval()\n",
    "surnames.set_split('test')\n",
    "\n",
    "batch_generator = generate_batches(surnames, batch_size = batch_size, device = device)\n",
    "\n",
    "val_loss = 0.\n",
    "val_acc = 0.\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_pred = classifier(x_in = batch_dict['x_data'], \n",
    "                        x_lens = batch_dict['x_len'])\n",
    "\n",
    "    # step 3. compute the loss\n",
    "    loss = loss_func(y_pred, batch_dict['y_data'])\n",
    "    val_loss += (loss.item() - val_loss) / (batch_index + 1)\n",
    "\n",
    "    # compute the accuracy\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_data'])\n",
    "    val_acc += (acc_t - val_acc) / (batch_index + 1)\n",
    "\n",
    "print('Validation loss', val_loss)\n",
    "print('Validation accuracy: ', val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad; the book got a test accuracy of 0.41, so we are not really far off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU\n",
    "We can easily substitute the elam RNN in our model for a GRU cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameClassifierGRU(nn.Module):\n",
    "    def __init__(self, embediing_size, num_embeddings, num_classes,\n",
    "                 rnn_hidden_size, batch_first = True, padding_idx = 0):\n",
    "        '''\n",
    "        num_embeddings: Number of characters to embedding (i.e., vocab size).\n",
    "        padding_idx: This index will be ignored by the later layers of the model\n",
    "        as it only indicates padding of the vectors. In our case, we chose the\n",
    "        padding token, <MASK>, to have index 0\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(num_embeddings = num_embeddings, \n",
    "                                embedding_dim = embediing_size,\n",
    "                                padding_idx = padding_idx)\n",
    "        # Only thing we changed\n",
    "        self.rnn = nn.GRU(input_size = embediing_size,\n",
    "                             hidden_size = rnn_hidden_size,\n",
    "                             batch_first = batch_first)\n",
    "        self.fc1 = nn.Linear(in_features = rnn_hidden_size,\n",
    "                         out_features = rnn_hidden_size)\n",
    "        self.fc2 = nn.Linear(in_features = rnn_hidden_size,\n",
    "                          out_features = num_classes)\n",
    "        \n",
    "    def forward(self, x_in, x_lens =  None, apply_softmax = False):\n",
    "        '''\n",
    "        x_in: should be of shape (batch, input_dim)\n",
    "        x_lens: the length of each sequence of the batch, used to \n",
    "            find the last hidden state\n",
    "        Returns:\n",
    "            tensor of shape (batch, output_dim) \n",
    "        '''\n",
    "        x_embedded = self.emb(x_in)\n",
    "        # GRU returns two tensors instead of just one\n",
    "        # In fact, the second is exactly the last hidden state\n",
    "        # and we could use it directly, but that would mean changing the code\n",
    "        y_out, _ = self.rnn(x_embedded)\n",
    "\n",
    "        if x_lens is not None:\n",
    "            y_out = column_gather(y_out, x_lens)\n",
    "        else:\n",
    "            # If no lens are proviced, we do not truncate\n",
    "            # the sequence, we take for last hidden state\n",
    "            # the actual last component of the vector\n",
    "            y_out = y_out[:, -1, :]\n",
    "\n",
    "        # We use a 2-layer MLP for classification\n",
    "        y_out = F.relu(self.fc1(F.dropout(y_out, 0.5)))\n",
    "        y_out = self.fc2(F.dropout(y_out, 0.5))\n",
    "\n",
    "        # Again, apply softmax only at eval, so to avoid numeric error\n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=1)\n",
    "\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Training loss 2.826365037759147\n",
      "Validation loss 2.707633447647095\n",
      "Validation accuracy:  0.25062500000000004\n",
      "Epoch:  1\n",
      "Training loss 2.497537271181741\n",
      "Validation loss 2.337283644676208\n",
      "Validation accuracy:  0.30000000000000004\n",
      "Epoch:  2\n",
      "Training loss 2.160082681973774\n",
      "Validation loss 2.117668299674988\n",
      "Validation accuracy:  0.34750000000000003\n",
      "Epoch:  3\n",
      "Training loss 1.999073917667071\n",
      "Validation loss 2.034896287918091\n",
      "Validation accuracy:  0.3475000000000001\n",
      "Epoch:  4\n",
      "Training loss 1.8988261272509894\n",
      "Validation loss 1.9682498121261598\n",
      "Validation accuracy:  0.37500000000000006\n",
      "Epoch:  5\n",
      "Training loss 1.798472915093104\n",
      "Validation loss 1.9375462055206298\n",
      "Validation accuracy:  0.38062499999999994\n",
      "Epoch:  6\n",
      "Training loss 1.7592316389083873\n",
      "Validation loss 1.8453709363937378\n",
      "Validation accuracy:  0.3937500000000001\n",
      "Epoch:  7\n",
      "Training loss 1.6778275226553283\n",
      "Validation loss 1.8217565441131593\n",
      "Validation accuracy:  0.3824999999999999\n",
      "Epoch:  8\n",
      "Training loss 1.6259264747301738\n",
      "Validation loss 1.818140139579773\n",
      "Validation accuracy:  0.39125000000000004\n",
      "Epoch:  9\n",
      "Training loss 1.5888129393259685\n",
      "Validation loss 1.824736571311951\n",
      "Validation accuracy:  0.421875\n",
      "Epoch:  10\n",
      "Training loss 1.5225056191285447\n",
      "Validation loss 1.726268858909607\n",
      "Validation accuracy:  0.41500000000000004\n",
      "Epoch:  11\n",
      "Training loss 1.5110535119970634\n",
      "Validation loss 1.795405111312866\n",
      "Validation accuracy:  0.41500000000000004\n",
      "Epoch:  12\n",
      "Training loss 1.4837513327598575\n",
      "Validation loss 1.811230421066284\n",
      "Validation accuracy:  0.40562500000000007\n",
      "Epoch:  13\n",
      "Training loss 1.4883614455660183\n",
      "Validation loss 1.771271071434021\n",
      "Validation accuracy:  0.43375\n",
      "Epoch:  14\n",
      "Training loss 1.4609337488810223\n",
      "Validation loss 1.6964724063873289\n",
      "Validation accuracy:  0.43\n",
      "Epoch:  15\n",
      "Training loss 1.4798199688394862\n",
      "Validation loss 1.7423437881469725\n",
      "Validation accuracy:  0.42625\n",
      "Epoch:  16\n",
      "Training loss 1.4485390086968735\n",
      "Validation loss 1.7940496301651\n",
      "Validation accuracy:  0.4175000000000001\n",
      "Epoch:  17\n",
      "Training loss 1.4605571692188577\n",
      "Validation loss 1.7447101926803588\n",
      "Validation accuracy:  0.41750000000000004\n",
      "Epoch:  18\n",
      "Training loss 1.463759405414264\n",
      "Validation loss 1.8113035035133365\n",
      "Validation accuracy:  0.43375\n",
      "Epoch:  19\n",
      "Training loss 1.443721095720927\n",
      "Validation loss 1.8504304933547973\n",
      "Validation accuracy:  0.4174999999999999\n",
      "Early stopped at epoch: 20\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "hidden_dimension = 5\n",
    "n_epochs = 50\n",
    "loss_func = nn.CrossEntropyLoss(surnames.class_weights)\n",
    "loss_func.to(device)\n",
    "batch_size = 64\n",
    "# Allows for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "embediing_size = 100\n",
    "num_embeddings = len(vectorizer.vocab)\n",
    "num_classes = surnames.class_weights.size(0)\n",
    "rnn_hidden_size = 64\n",
    "\n",
    "classifier = SurnameClassifierGRU(embediing_size, num_embeddings, num_classes,\n",
    "                 rnn_hidden_size, batch_first = True, padding_idx = 0)\n",
    "classifier.to(device)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr = lr)\n",
    "# Multiplies the learning rate by `gamma` every `step_size` epoch\n",
    "scheduler = StepLR(optimizer, step_size = 10, gamma = 0.1)\n",
    "\n",
    "early_stopping = EarlyStopping(min_delta = 0, save = 'best_rnn_model.pt')\n",
    "\n",
    "for epoch in range(n_epochs): \n",
    "    surnames.set_split('train')\n",
    "    batch_generator = generate_batches(surnames, batch_size = batch_size, device = device)\n",
    "    running_loss = 0.\n",
    "    classifier.train()\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # 1. Zero the gradient\n",
    "        classifier.zero_grad()\n",
    "\n",
    "        # 2. Prediction\n",
    "        y_pred = classifier(x_in = batch_dict['x_data'],\n",
    "                            x_lens = batch_dict['x_len'])\n",
    "\n",
    "        # 3. Compute loss\n",
    "        loss = loss_func(y_pred, batch_dict['y_data'])\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "\n",
    "        # 4. Backpropagate\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Optimize\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluation part, we don't want paramereres to change\n",
    "    classifier.eval()\n",
    "    surnames.set_split('val')\n",
    "\n",
    "    batch_generator = generate_batches(surnames, batch_size = batch_size, device = device)\n",
    "    \n",
    "    val_loss = 0.\n",
    "    val_acc = 0.\n",
    "    classifier.eval()\n",
    "\n",
    "    \n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # compute the output\n",
    "        y_pred = classifier(x_in = batch_dict['x_data'], \n",
    "                            x_lens = batch_dict['x_len'])\n",
    "\n",
    "        # step 3. compute the loss\n",
    "        loss = loss_func(y_pred, batch_dict['y_data'])\n",
    "        val_loss += (loss.item() - val_loss) / (batch_index + 1)\n",
    "\n",
    "        # compute the accuracy\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict['y_data'])\n",
    "        val_acc += (acc_t - val_acc) / (batch_index + 1)\n",
    "\n",
    "    # Updates early stop\n",
    "    early_stopping(val_loss, classifier)\n",
    "    if early_stopping.flag:\n",
    "        print('Early stopped at epoch:', epoch)\n",
    "        break\n",
    "\n",
    "    # Updates scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    print('Epoch: ', epoch)\n",
    "    print('Training loss', running_loss)\n",
    "    print('Validation loss', val_loss)\n",
    "    print('Validation accuracy: ', val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss 1.741636519432068\n",
      "Validation accuracy:  0.4225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7532/1275012218.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  classifier.load_state_dict(torch.load(\"best_rnn_model.pt\"))\n"
     ]
    }
   ],
   "source": [
    "classifier.load_state_dict(torch.load(\"best_rnn_model.pt\"))\n",
    "classifier.eval()\n",
    "\n",
    "# Evaluation part, we don't want paramereres to change\n",
    "classifier.eval()\n",
    "surnames.set_split('test')\n",
    "\n",
    "batch_generator = generate_batches(surnames, batch_size = batch_size, device = device)\n",
    "\n",
    "val_loss = 0.\n",
    "val_acc = 0.\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_pred = classifier(x_in = batch_dict['x_data'], \n",
    "                        x_lens = batch_dict['x_len'])\n",
    "\n",
    "    # step 3. compute the loss\n",
    "    loss = loss_func(y_pred, batch_dict['y_data'])\n",
    "    val_loss += (loss.item() - val_loss) / (batch_index + 1)\n",
    "\n",
    "    # compute the accuracy\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_data'])\n",
    "    val_acc += (acc_t - val_acc) / (batch_index + 1)\n",
    "\n",
    "print('Validation loss', val_loss)\n",
    "print('Validation accuracy: ', val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So GRU does improve the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
