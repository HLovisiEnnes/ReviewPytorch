{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today's notebook will be significantly shorter than other's as we will play a little with a library that I have never used before: DeepEval. Overall, DeepEval is implements some LLM-as-a-Judge models, that is, they use LLMs to evaluate the performance of other LLMs. It has several models for this, but we will mainly focus on G-Eval, which seems to be their most used module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G-Eval\n",
    "\n",
    "G-Rval consists of a LLM-as-a-Judge model which, given an evaluation instance, automatically generate a Chain of Thoughts (CoT) to judge how well the evalued LLM performed. This can be done either in respect to itself or, more generally, in respect to an expected ouptut sentence. With this, we expect that the evaluation will be more sensitive to overall semantic and syntatic differences of the LLM output with respect to the expected answer.\n",
    "For now, we will play with some fake outputs, but later, we will use Hugging Face to evaluate real models. We will be using Ollama because it is free :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/hlovisie/venvs/torch-cuda/lib/python3.11/site-packages/rich/live.py:256: UserWarning: install \"ipywidgets\" \n",
       "for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/hlovisie/venvs/torch-cuda/lib/python3.11/site-packages/rich/live.py:256: UserWarning: install \"ipywidgets\" \n",
       "for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3 The actual output 'Tom, the cat' does not directly answer the question 'Who ran up the tree?' and is mostly correct in relevance to the context but lacks specificity regarding who ran up the tree.\n"
     ]
    }
   ],
   "source": [
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.models import OllamaModel\n",
    "from deepeval.metrics.g_eval import Rubric\n",
    "\n",
    "\n",
    "model = OllamaModel(\n",
    "    model = \"deepseek-r1:1.5b\"\n",
    ")\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    context =  [\"The dog chased the cat called Tom, up the tree\"],\n",
    "    input = \"Who ran up the tree?\",\n",
    "    actual_output = \"Tom, the cat.\",\n",
    "    expected_output = \"The cat.\")\n",
    "\n",
    "coherence_metric = GEval(\n",
    "    name = \"Coherence\",\n",
    "    criteria = \"Coherence - the collective quality of all sentences in the actual output\",\n",
    "    evaluation_params = [LLMTestCaseParams.INPUT, \n",
    "                         LLMTestCaseParams.ACTUAL_OUTPUT, \n",
    "                         LLMTestCaseParams.EXPECTED_OUTPUT, \n",
    "                         LLMTestCaseParams.CONTEXT],\n",
    "    model = model,\n",
    "    rubric=[\n",
    "        Rubric(score_range=(0,2), expected_outcome=\"Factually incorrect.\"),\n",
    "        Rubric(score_range=(3,6), expected_outcome=\"Mostly correct.\"),\n",
    "        Rubric(score_range=(7,9), expected_outcome=\"Correct but missing minor details.\"),\n",
    "        Rubric(score_range=(10,10), expected_outcome=\"100% correct.\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "coherence_metric.measure(test_case)\n",
    "print(coherence_metric.score, coherence_metric.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the actual CoT that the model created to evaluate this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Assess Input Alignment with Expected Output',\n",
       " 'Evaluate Clarity and Relevance of Actual Output',\n",
       " 'Ensure Context Supports Both Input and Expected Output',\n",
       " 'Check Coherence by Ensuring Internal Flow and Connections']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence_metric.evaluation_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for comparison, we can manually force some CoT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7 The actual output 'Tom, the cat.' is mostly correct as it accurately describes who ran up the tree. It omits 'Tom' but includes his name, which is a minor detail. The context and expected output are consistent, so the score is 7-9.\n"
     ]
    }
   ],
   "source": [
    "coherence_metric = GEval(\n",
    "    name = \"Coherence\",\n",
    "    criteria = \"Coherence - the collective quality of all sentences in the actual output\",\n",
    "    evaluation_params = [LLMTestCaseParams.INPUT, \n",
    "                         LLMTestCaseParams.ACTUAL_OUTPUT, \n",
    "                         LLMTestCaseParams.EXPECTED_OUTPUT, \n",
    "                         LLMTestCaseParams.CONTEXT],\n",
    "    model = model,\n",
    "    rubric=[\n",
    "        Rubric(score_range=(0,2), expected_outcome=\"Factually incorrect.\"),\n",
    "        Rubric(score_range=(3,6), expected_outcome=\"Mostly correct.\"),\n",
    "        Rubric(score_range=(7,9), expected_outcome=\"Correct but missing minor details.\"),\n",
    "        Rubric(score_range=(10,10), expected_outcome=\"100% correct.\"),\n",
    "    ],\n",
    "    evaluation_steps=[\n",
    "        \"Check whether the facts in 'actual output' contradicts any facts in 'expected output'\",\n",
    "        \"You should not penalize omission of detail\",\n",
    "        \"Vague language, or contradicting OPINIONS, are OK\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "coherence_metric.measure(test_case)\n",
    "print(coherence_metric.score, coherence_metric.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the model performance improves!\n",
    "\n",
    "\n",
    "Let us see how this works for some summarization task. But for that, we will use a real model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline('summarization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us apply this model for some summarization task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I got some random Wikipedia entry https://en.wikipedia.org/wiki/Ovalipes_catharus\n",
    "text =  '''\n",
    "Ovalipes catharus has an oval-shaped, streamlined, and slightly grainy carapace with five large, \n",
    "sawtooth-like projections to either side of the eyes and four smaller ones at the front.\n",
    "The dorsal carapace has two large, maroon eyespots at the rear, two smaller eyespots near the front, and cervical grooves which form a \n",
    "butterfly-shaped mark near the centre.\n",
    "It is overall sandy grey with orange-red highlights and dotted with small, brown spots.\n",
    "The crab's underside is white, and its rear legs – which are flattened and function as swimming paddles – have a purplish tinge.\n",
    "Unlike about half of Ovalipes species, O. catharus' body exhibits no iridescence.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ovalipes catharus has an oval-shaped, streamlined, and slightly grainy carapace with five large, sawtooth-like projections to either side of the eyes and four smaller ones at the front . It is overall sandy grey with orange-red highlights and dotted with small, brown spots .\n"
     ]
    }
   ],
   "source": [
    "summary = summarizer(text)[0]['summary_text']\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/hlovisie/venvs/torch-cuda/lib/python3.11/site-packages/rich/live.py:256: UserWarning: install \"ipywidgets\" \n",
       "for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/hlovisie/venvs/torch-cuda/lib/python3.11/site-packages/rich/live.py:256: UserWarning: install \"ipywidgets\" \n",
       "for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 The actual output is a concise summary of the input, accurately describing the carapace structure and features without unnecessary details.\n"
     ]
    }
   ],
   "source": [
    "test_case = LLMTestCase(\n",
    "    input = text,\n",
    "    actual_output = summary)\n",
    "\n",
    "coherence_metric = GEval(\n",
    "    name = \"Coherence\",\n",
    "    criteria = \"Determine how good a summary the 'actual output' is to the 'input'\",\n",
    "    evaluation_params = [LLMTestCaseParams.INPUT, \n",
    "                         LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    model = model\n",
    ")\n",
    "\n",
    "coherence_metric.measure(test_case)\n",
    "print(coherence_metric.score, coherence_metric.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, the model seems to be very well evaluated! Let us see some chaos, I'll take now the Wikipedia article of another animal in the same family."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_text = '''\n",
    "The carapace of O. ocellatus is slightly wider than long, at 8.9 centimetres (3.5 in) wide, and 7.5 cm (3.0 in) long.\n",
    "The carapace is yellow-grey or light purplish, with \"leopardlike clusters of purple dots\". \n",
    "It exhibits a limited iridescence as a form of signalling.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/hlovisie/venvs/torch-cuda/lib/python3.11/site-packages/rich/live.py:256: UserWarning: install \"ipywidgets\" \n",
       "for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/hlovisie/venvs/torch-cuda/lib/python3.11/site-packages/rich/live.py:256: UserWarning: install \"ipywidgets\" \n",
       "for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 The actual output is concise but could be clearer by using 'catharus' instead of 'Ocellatus' for brevity. It also includes more descriptive language that might seem redundant compared to the input.\n"
     ]
    }
   ],
   "source": [
    "test_case = LLMTestCase(\n",
    "    input = fake_text,\n",
    "    actual_output = summary)\n",
    "\n",
    "coherence_metric = GEval(\n",
    "    name = \"Coherence\",\n",
    "    criteria = \"Determine how good a summary the 'actual output' is to the 'input'\",\n",
    "    evaluation_params = [LLMTestCaseParams.INPUT, \n",
    "                         LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    model = model\n",
    ")\n",
    "\n",
    "coherence_metric.measure(test_case)\n",
    "print(coherence_metric.score, coherence_metric.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, it was able to tell that there's something wrong. In fact, we enforce a lower grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/hlovisie/venvs/torch-cuda/lib/python3.11/site-packages/rich/live.py:256: UserWarning: install \"ipywidgets\" \n",
       "for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/hlovisie/venvs/torch-cuda/lib/python3.11/site-packages/rich/live.py:256: UserWarning: install \"ipywidgets\" \n",
       "for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 The input focuses on O. ocellatus while the output is about O. ovalipes, a different species. The main idea of the input is captured in the output but with differences in focus.\n"
     ]
    }
   ],
   "source": [
    "test_case = LLMTestCase(\n",
    "    input = fake_text,\n",
    "    actual_output = summary)\n",
    "\n",
    "coherence_metric = GEval(\n",
    "    name = \"Coherence\",\n",
    "    criteria = \"Determine how good a summary the 'actual output' is to the 'input'. Penalize the model if hallucinating too much.\",\n",
    "    evaluation_params = [LLMTestCaseParams.INPUT, \n",
    "                         LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    model = model\n",
    ")\n",
    "\n",
    "coherence_metric.measure(test_case)\n",
    "print(coherence_metric.score, coherence_metric.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepEvals has several other metrics and appliances that I didn't have the time to cover here. Nonetheless, I'd still like to compare our LLM-as-a-Judge with the two classical statistical metrics for text summarization, BLEU and ROUGE. I'll be using Hugging Face's implementation of these metrics as that is the syntax I am more used to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.3857500553435857,\n",
       " 'precisions': [0.4,\n",
       "  0.3949579831932773,\n",
       "  0.3813559322033898,\n",
       "  0.36752136752136755],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 2.5,\n",
       " 'translation_length': 120,\n",
       " 'reference_length': 48}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"bleu\")  \n",
    "metric.compute(predictions = [text], references = [summary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.0,\n",
       " 'precisions': [0.225, 0.025210084033613446, 0.0, 0.0],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 2.1052631578947367,\n",
       " 'translation_length': 120,\n",
       " 'reference_length': 57}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.compute(predictions = [text], references = [fake_summary])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': np.float64(0.5882352941176471),\n",
       " 'rouge2': np.float64(0.5695364238410596),\n",
       " 'rougeL': np.float64(0.5882352941176471),\n",
       " 'rougeLsum': np.float64(0.5882352941176471)}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = evaluate.load(\"rouge\")  \n",
    "metric.compute(predictions = [text], references = [summary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': np.float64(0.2802547770700637),\n",
       " 'rouge2': np.float64(0.0),\n",
       " 'rougeL': np.float64(0.11464968152866241),\n",
       " 'rougeLsum': np.float64(0.1910828025477707)}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.compute(predictions = [text], references = [fake_summary])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So they also indicate not too good matches and they also indicate that the fake summary is, indeed, fake. Nonethless, they seem much less confident than LLMs-as-judges."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
